{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def _is_whitespace(c):\n",
    "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text = \"\"\"Beyonc\\u00e9 attended St. Mary's Elementary School in Fredericksburg, Texas, where she enrolled in dance classes. Her singing talent was discovered when dance instructor Darlette Johnson began humming a song and she finished it, able to hit the high-pitched notes. Beyonc\\u00e9's interest in music and performing continued after winning a school talent show at age seven, singing John Lennon's \\\"Imagine\\\" to beat 15/16-year-olds. In fall of 1990, Beyonc\\u00e9 enrolled in Parker Elementary School, a music magnet school in Houston, where she would perform with the school's choir. She also attended the High School for the Performing and Visual Arts and later Alief Elsik High School. Beyonc\\u00e9 was also a member of the choir at St. John's United Methodist Church as a soloist for two years.\"\"\"\n",
    "# context_text = 'she is a little pig barbosaa'\n",
    "question_text = \"Fredericksburg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "a = re.search(question_text, context_text)\n",
    "print(a.start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_word_offset(context):\n",
    "    doc_tokens = []\n",
    "    char_to_word_offset = []\n",
    "    prev_is_whitespace = True\n",
    "\n",
    "    for c in context_text:\n",
    "        if _is_whitespace(c):\n",
    "            prev_is_whitespace = True\n",
    "        else:\n",
    "            if prev_is_whitespace:\n",
    "                doc_tokens.append(c)\n",
    "            else:\n",
    "                doc_tokens[-1] += c\n",
    "            prev_is_whitespace = False\n",
    "        char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "    return doc_tokens, char_to_word_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'the billionaire mogul'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'billionaire', 'mogul']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_tokens.index('the')\n",
    "doc_tokens.index('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokens, char_to_word_offset = map_word_offset(context_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_position(context, answer):\n",
    "    doc_tokens, char_to_word_offset = map_word_offset(context)\n",
    "    # Take top 3 words for seraching the position of the start position, avoiding same word diff index situation.\n",
    "    targets = answer.split()[:3]\n",
    "    first_idx = 0\n",
    "    second_idx = 0\n",
    "    third_idx = 0\n",
    "    position = 0\n",
    "    print(targets)\n",
    "    for word in doc_tokens:\n",
    "        if word == targets[0]:\n",
    "            fisrt_idx = doc_tokens.index(word)\n",
    "            print(first_idx)\n",
    "            position = char_to_word_offset.index(fisrt_idx)\n",
    "        elif word == targets[1]:\n",
    "            second_idx = doc_tokens.index(word)\n",
    "            print(second_idx)\n",
    "        elif word == targets[2]:\n",
    "            third_idx = doc_tokens.index(word)\n",
    "            print(third_idx)\n",
    "        else:\n",
    "            pass\n",
    "    if (first_idx + second_idx + third_idx) / 3 == second_idx:\n",
    "        return position  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#string = 'This is laughing laugh'\n",
    "a = re.search(question_text, context_text)\n",
    "\n",
    "#a = re.search(r'\\b(laugh)\\b', context_text)\n",
    "print(a.start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n",
      "209\n"
     ]
    }
   ],
   "source": [
    "target = 'GM'\n",
    "for word in doc_tokens:\n",
    "    if target == word:\n",
    "        index = doc_tokens.index(word)\n",
    "        position = char_to_word_offset.index(index)\n",
    "        print(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "531"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(char_to_word_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = (1, 2), (3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3\n",
      "2 4\n"
     ]
    }
   ],
   "source": [
    "for start, end in zip(start, end):\n",
    "    print(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)\n",
    "start_position = 5\n",
    "end_position = 30\n",
    "\n",
    "tok_to_orig_index = []\n",
    "orig_to_tok_index = []\n",
    "all_doc_tokens = []\n",
    "\n",
    "for (i, token) in enumerate(doc_tokens):\n",
    "    orig_to_tok_index.append(len(all_doc_tokens))\n",
    "    sub_tokens = tokenizer.tokenize(token)\n",
    "    for sub_token in sub_tokens:\n",
    "        tok_to_orig_index.append(i)\n",
    "        all_doc_tokens.append(sub_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 5, 5, 5]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_to_orig_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_to_tok_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁she', '▁is', '▁a', '▁little', '▁pig', '▁', 'barb', 'osa', 'a']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer, orig_answer_text):\n",
    "    \"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"\n",
    "    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n",
    "\n",
    "    for new_start in range(input_start, input_end + 1):\n",
    "        for new_end in range(input_end, new_start - 1, -1):\n",
    "            text_span = \" \".join(doc_tokens[new_start : (new_end + 1)])\n",
    "            if text_span == tok_answer_text:\n",
    "                return (new_start, new_end)\n",
    "\n",
    "    return (input_start, input_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_improve_answer_span(all_doc_tokens, 3, 5, tokenizer, \"i am jack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.3077, grad_fn=<DivBackward0>),)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁she', '▁is', '▁a', '▁little', '▁pig', '▁', 'barb', 'osa', 'a']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_doc_tokens = all_doc_tokens\n",
    "span_doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_query = tokenizer.encode(question_text, add_special_tokens=False, max_length=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "spans = []\n",
    "doc_stride = 24\n",
    "max_seq_length = 48\n",
    "sequence_added_tokens = 2\n",
    "while len(spans) * 12 < len(all_doc_tokens):\n",
    "\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "            truncated_query if tokenizer.padding_side == \"right\" else span_doc_tokens,\n",
    "            span_doc_tokens if tokenizer.padding_side == \"right\" else truncated_query,\n",
    "            max_length=24,\n",
    "            return_overflowing_tokens=True,\n",
    "            pad_to_max_length=True,\n",
    "            stride=24 - 128 - len(truncated_query) - sequence_pair_added_tokens,\n",
    "            truncation_strategy=\"only_second\" if tokenizer.padding_side == \"right\" else \"only_first\",\n",
    "            return_token_type_ids=True,\n",
    "        )\n",
    "    paragraph_len = min(\n",
    "            len(all_doc_tokens) - len(spans) * doc_stride,\n",
    "            max_seq_length - len(truncated_query) - sequence_pair_added_tokens,\n",
    "        )\n",
    "\n",
    "    if tokenizer.pad_token_id in encoded_dict[\"input_ids\"]:\n",
    "        if tokenizer.padding_side == \"right\":\n",
    "            non_padded_ids = encoded_dict[\"input_ids\"][: encoded_dict[\"input_ids\"].index(tokenizer.pad_token_id)]\n",
    "        else:\n",
    "            last_padding_id_position = (\n",
    "                len(encoded_dict[\"input_ids\"]) - 1 - encoded_dict[\"input_ids\"][::-1].index(tokenizer.pad_token_id)\n",
    "                )\n",
    "            non_padded_ids = encoded_dict[\"input_ids\"][last_padding_id_position + 1 :]\n",
    "\n",
    "    else:\n",
    "        non_padded_ids = encoded_dict[\"input_ids\"]\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)\n",
    "\n",
    "    token_to_orig_map = {}\n",
    "    for i in range(paragraph_len):\n",
    "        index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == \"right\" else i\n",
    "        token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]\n",
    "\n",
    "    encoded_dict[\"paragraph_len\"] = paragraph_len\n",
    "    encoded_dict[\"tokens\"] = tokens\n",
    "    encoded_dict[\"token_to_orig_map\"] = token_to_orig_map\n",
    "    encoded_dict[\"truncated_query_with_special_tokens_length\"] = len(truncated_query) + sequence_added_tokens\n",
    "    encoded_dict[\"token_is_max_context\"] = {}\n",
    "    encoded_dict[\"start\"] = len(spans) * doc_stride\n",
    "    encoded_dict[\"length\"] = paragraph_len\n",
    "\n",
    "    spans.append(encoded_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': [5,\n",
       "   5,\n",
       "   5,\n",
       "   5,\n",
       "   5,\n",
       "   5,\n",
       "   5,\n",
       "   5,\n",
       "   85,\n",
       "   27,\n",
       "   24,\n",
       "   293,\n",
       "   13200,\n",
       "   17,\n",
       "   19192,\n",
       "   9155,\n",
       "   101,\n",
       "   4,\n",
       "   17,\n",
       "   150,\n",
       "   569,\n",
       "   12323,\n",
       "   4,\n",
       "   3],\n",
       "  'token_type_ids': [3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   2],\n",
       "  'attention_mask': [0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  'paragraph_len': 9,\n",
       "  'tokens': ['▁she',\n",
       "   '▁is',\n",
       "   '▁a',\n",
       "   '▁little',\n",
       "   '▁pig',\n",
       "   '▁',\n",
       "   'barb',\n",
       "   'osa',\n",
       "   'a',\n",
       "   '<sep>',\n",
       "   '▁',\n",
       "   'i',\n",
       "   '▁am',\n",
       "   '▁jack',\n",
       "   '<sep>',\n",
       "   '<cls>'],\n",
       "  'token_to_orig_map': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 5, 7: 5, 8: 5},\n",
       "  'truncated_query_with_special_tokens_length': 6,\n",
       "  'token_is_max_context': {},\n",
       "  'start': 0,\n",
       "  'length': 9}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadExample(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        qas_id,\n",
    "        question_text,\n",
    "        context_text,\n",
    "        answer_text,\n",
    "        start_position_character,\n",
    "        title,\n",
    "        answers=[],\n",
    "        is_impossible=False,\n",
    "    ):\n",
    "        self.qas_id = qas_id\n",
    "        self.question_text = question_text\n",
    "        self.context_text = context_text\n",
    "        self.answer_text = answer_text\n",
    "        self.title = title\n",
    "        self.is_impossible = is_impossible\n",
    "        self.answers = answers\n",
    "\n",
    "        self.start_position, self.end_position = 0, 0\n",
    "\n",
    "        doc_tokens = []\n",
    "        char_to_word_offset = []\n",
    "        prev_is_whitespace = True\n",
    "\n",
    "        # Split on whitespace so that different tokens may be attributed to their original position.\n",
    "        for c in self.context_text:\n",
    "            if _is_whitespace(c):\n",
    "                prev_is_whitespace = True\n",
    "            else:\n",
    "                if prev_is_whitespace:\n",
    "                    doc_tokens.append(c)\n",
    "                else:\n",
    "                    doc_tokens[-1] += c\n",
    "                prev_is_whitespace = False\n",
    "            char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "\n",
    "        self.doc_tokens = doc_tokens\n",
    "        self.char_to_word_offset = char_to_word_offset\n",
    "\n",
    "        # Start and end positions only has a value during evaluation.\n",
    "        if start_position_character is not None and not is_impossible:\n",
    "            self.start_position = char_to_word_offset[start_position_character]\n",
    "            self.end_position = char_to_word_offset[\n",
    "                min(start_position_character + len(answer_text) - 1, len(char_to_word_offset) - 1)\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample(\n",
    "        question: Union[str, List[str]], context: Union[str, List[str]]\n",
    "    ) -> Union[SquadExample, List[SquadExample]]:\n",
    "        \"\"\"\n",
    "        QuestionAnsweringPipeline leverages the SquadExample/SquadFeatures internally.\n",
    "        This helper method encapsulate all the logic for converting question(s) and context(s) to SquadExample(s).\n",
    "        We currently support extractive question answering.\n",
    "        Arguments:\n",
    "             question: (str, List[str]) The question to be ask for the associated context\n",
    "             context: (str, List[str]) The context in which we will look for the answer.\n",
    "        Returns:\n",
    "            SquadExample initialized with the corresponding question and context.\n",
    "        \"\"\"\n",
    "        if isinstance(question, list):\n",
    "            return [SquadExample(None, q, c, None, None, None) for q, c in zip(question, context)]\n",
    "        else:\n",
    "            return SquadExample(None, question, context, None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = create_sample(question_text, context_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 28,\n",
       " 28,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 33,\n",
       " 33,\n",
       " 33,\n",
       " 33,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 35,\n",
       " 35,\n",
       " 35,\n",
       " 35,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 37,\n",
       " 37,\n",
       " 37,\n",
       " 37,\n",
       " 37,\n",
       " 37,\n",
       " 37,\n",
       " 37,\n",
       " 37,\n",
       " 37,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 39,\n",
       " 39,\n",
       " 39,\n",
       " 39,\n",
       " 39,\n",
       " 39,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 41,\n",
       " 41,\n",
       " 41,\n",
       " 41,\n",
       " 42,\n",
       " 42,\n",
       " 42,\n",
       " 42,\n",
       " 42,\n",
       " 42,\n",
       " 42,\n",
       " 42,\n",
       " 42,\n",
       " 42,\n",
       " 42,\n",
       " 42,\n",
       " 43,\n",
       " 43,\n",
       " 43,\n",
       " 43,\n",
       " 43,\n",
       " 43,\n",
       " 43,\n",
       " 43,\n",
       " 43,\n",
       " 43,\n",
       " 43,\n",
       " 44,\n",
       " 44,\n",
       " 44,\n",
       " 44,\n",
       " 44,\n",
       " 44,\n",
       " 44,\n",
       " 44,\n",
       " 44,\n",
       " 45,\n",
       " 45,\n",
       " 45,\n",
       " 45,\n",
       " 45,\n",
       " 45,\n",
       " 45,\n",
       " 45,\n",
       " 45,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 47,\n",
       " 47,\n",
       " 47,\n",
       " 47,\n",
       " 47,\n",
       " 48,\n",
       " 48,\n",
       " 48,\n",
       " 49,\n",
       " 49,\n",
       " 49,\n",
       " 49,\n",
       " 49,\n",
       " 49,\n",
       " 49,\n",
       " 49,\n",
       " 49,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 51,\n",
       " 51,\n",
       " 51,\n",
       " 51,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 52,\n",
       " 53,\n",
       " 53,\n",
       " 53,\n",
       " 53,\n",
       " 53,\n",
       " 53,\n",
       " 53,\n",
       " 53,\n",
       " 54,\n",
       " 54,\n",
       " 54,\n",
       " 55,\n",
       " 55,\n",
       " 56,\n",
       " 56,\n",
       " 56,\n",
       " 56,\n",
       " 56,\n",
       " 56,\n",
       " 56,\n",
       " 57,\n",
       " 57,\n",
       " 57,\n",
       " 57,\n",
       " 57,\n",
       " 58,\n",
       " 58,\n",
       " 58,\n",
       " 58,\n",
       " 58,\n",
       " 58,\n",
       " 58,\n",
       " 58,\n",
       " 59,\n",
       " 59,\n",
       " 59,\n",
       " 59,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 61,\n",
       " 62,\n",
       " 62,\n",
       " 62,\n",
       " 62,\n",
       " 62,\n",
       " 62,\n",
       " 62,\n",
       " 62,\n",
       " 62,\n",
       " 62,\n",
       " 63,\n",
       " 63,\n",
       " 63,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 64,\n",
       " 65,\n",
       " 65,\n",
       " 65,\n",
       " 65,\n",
       " 65,\n",
       " 66,\n",
       " 66,\n",
       " 66,\n",
       " 67,\n",
       " 67,\n",
       " 67,\n",
       " 67,\n",
       " 67,\n",
       " 67,\n",
       " 67,\n",
       " 67,\n",
       " 67,\n",
       " 67,\n",
       " 68,\n",
       " 68,\n",
       " 68,\n",
       " 68,\n",
       " 68,\n",
       " 68,\n",
       " 68,\n",
       " 68,\n",
       " 69,\n",
       " 69,\n",
       " 69,\n",
       " 69,\n",
       " 69,\n",
       " 69,\n",
       " 69,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 71,\n",
       " 71,\n",
       " 71,\n",
       " 71,\n",
       " 71,\n",
       " 71,\n",
       " 72,\n",
       " 72,\n",
       " 72,\n",
       " 72,\n",
       " 72,\n",
       " 72,\n",
       " 72,\n",
       " 72,\n",
       " 72,\n",
       " 72,\n",
       " 72,\n",
       " 72,\n",
       " 73,\n",
       " 73,\n",
       " 73,\n",
       " 73,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 75,\n",
       " 75,\n",
       " 75,\n",
       " 76,\n",
       " 76,\n",
       " 76,\n",
       " 76,\n",
       " 76,\n",
       " 76,\n",
       " 76,\n",
       " 76,\n",
       " 76,\n",
       " 76,\n",
       " 76,\n",
       " 76,\n",
       " 77,\n",
       " 77,\n",
       " 77,\n",
       " 77,\n",
       " 77,\n",
       " 77,\n",
       " 77,\n",
       " 77,\n",
       " 77,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 79,\n",
       " 79,\n",
       " 79,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81,\n",
       " 81]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.char_to_word_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a.question_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_convert_example_to_features(example, max_seq_length, doc_stride, max_query_length, is_training):\n",
    "    features = []\n",
    "    if is_training and not example.is_impossible:\n",
    "        # Get start and end position\n",
    "        start_position = example.start_position\n",
    "        end_position = example.end_position\n",
    "\n",
    "        # If the answer cannot be found in the text, then skip this example.\n",
    "        actual_text = \" \".join(example.doc_tokens[start_position : (end_position + 1)])\n",
    "        cleaned_answer_text = \" \".join(whitespace_tokenize(example.answer_text))\n",
    "        if actual_text.find(cleaned_answer_text) == -1:\n",
    "            logger.warning(\"Could not find answer: '%s' vs. '%s'\", actual_text, cleaned_answer_text)\n",
    "            return []\n",
    "\n",
    "    tok_to_orig_index = []\n",
    "    orig_to_tok_index = []\n",
    "    all_doc_tokens = []\n",
    "    for (i, token) in enumerate(example.doc_tokens):\n",
    "        orig_to_tok_index.append(len(all_doc_tokens))\n",
    "        sub_tokens = tokenizer.tokenize(token)\n",
    "        for sub_token in sub_tokens:\n",
    "            tok_to_orig_index.append(i)\n",
    "            all_doc_tokens.append(sub_token)\n",
    "\n",
    "    if is_training and not example.is_impossible:\n",
    "        tok_start_position = orig_to_tok_index[example.start_position]\n",
    "        if example.end_position < len(example.doc_tokens) - 1:\n",
    "            tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n",
    "        else:\n",
    "            tok_end_position = len(all_doc_tokens) - 1\n",
    "\n",
    "        (tok_start_position, tok_end_position) = _improve_answer_span(\n",
    "            all_doc_tokens, tok_start_position, tok_end_position, tokenizer, example.answer_text\n",
    "        )\n",
    "\n",
    "    spans = []\n",
    "\n",
    "    truncated_query = tokenizer.encode(example.question_text, add_special_tokens=False, max_length=max_query_length)\n",
    "    sequence_added_tokens = (\n",
    "        tokenizer.max_len - tokenizer.max_len_single_sentence + 1\n",
    "        if \"roberta\" in str(type(tokenizer)) or \"camembert\" in str(type(tokenizer))\n",
    "        else tokenizer.max_len - tokenizer.max_len_single_sentence\n",
    "    )\n",
    "    sequence_pair_added_tokens = tokenizer.max_len - tokenizer.max_len_sentences_pair\n",
    "\n",
    "    span_doc_tokens = all_doc_tokens\n",
    "    while len(spans) * doc_stride < len(all_doc_tokens):\n",
    "\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            truncated_query if tokenizer.padding_side == \"right\" else span_doc_tokens,\n",
    "            span_doc_tokens if tokenizer.padding_side == \"right\" else truncated_query,\n",
    "            max_length=max_seq_length,\n",
    "            return_overflowing_tokens=True,\n",
    "            pad_to_max_length=True,\n",
    "            stride=max_seq_length - doc_stride - len(truncated_query) - sequence_pair_added_tokens,\n",
    "            truncation_strategy=\"only_second\" if tokenizer.padding_side == \"right\" else \"only_first\",\n",
    "            return_token_type_ids=True,\n",
    "        )\n",
    "\n",
    "        paragraph_len = min(\n",
    "            len(all_doc_tokens) - len(spans) * doc_stride,\n",
    "            max_seq_length - len(truncated_query) - sequence_pair_added_tokens,\n",
    "        )\n",
    "\n",
    "        if tokenizer.pad_token_id in encoded_dict[\"input_ids\"]:\n",
    "            if tokenizer.padding_side == \"right\":\n",
    "                non_padded_ids = encoded_dict[\"input_ids\"][: encoded_dict[\"input_ids\"].index(tokenizer.pad_token_id)]\n",
    "            else:\n",
    "                last_padding_id_position = (\n",
    "                    len(encoded_dict[\"input_ids\"]) - 1 - encoded_dict[\"input_ids\"][::-1].index(tokenizer.pad_token_id)\n",
    "                )\n",
    "                non_padded_ids = encoded_dict[\"input_ids\"][last_padding_id_position + 1 :]\n",
    "\n",
    "        else:\n",
    "            non_padded_ids = encoded_dict[\"input_ids\"]\n",
    "\n",
    "        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)\n",
    "\n",
    "        token_to_orig_map = {}\n",
    "        for i in range(paragraph_len):\n",
    "            index = len(truncated_query) + sequence_added_tokens + i if tokenizer.padding_side == \"right\" else i\n",
    "            token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]\n",
    "\n",
    "        encoded_dict[\"paragraph_len\"] = paragraph_len\n",
    "        encoded_dict[\"tokens\"] = tokens\n",
    "        encoded_dict[\"token_to_orig_map\"] = token_to_orig_map\n",
    "        encoded_dict[\"truncated_query_with_special_tokens_length\"] = len(truncated_query) + sequence_added_tokens\n",
    "        encoded_dict[\"token_is_max_context\"] = {}\n",
    "        encoded_dict[\"start\"] = len(spans) * doc_stride\n",
    "        encoded_dict[\"length\"] = paragraph_len\n",
    "\n",
    "        spans.append(encoded_dict)\n",
    "\n",
    "        if \"overflowing_tokens\" not in encoded_dict:\n",
    "            break\n",
    "        span_doc_tokens = encoded_dict[\"overflowing_tokens\"]\n",
    "\n",
    "    for doc_span_index in range(len(spans)):\n",
    "        for j in range(spans[doc_span_index][\"paragraph_len\"]):\n",
    "            is_max_context = _new_check_is_max_context(spans, doc_span_index, doc_span_index * doc_stride + j)\n",
    "            index = (\n",
    "                j\n",
    "                if tokenizer.padding_side == \"left\"\n",
    "                else spans[doc_span_index][\"truncated_query_with_special_tokens_length\"] + j\n",
    "            )\n",
    "            spans[doc_span_index][\"token_is_max_context\"][index] = is_max_context\n",
    "\n",
    "    for span in spans:\n",
    "        # Identify the position of the CLS token\n",
    "        cls_index = span[\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "\n",
    "        # p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)\n",
    "        # Original TF implem also keep the classification token (set to 0) (not sure why...)\n",
    "        p_mask = np.array(span[\"token_type_ids\"])\n",
    "\n",
    "        p_mask = np.minimum(p_mask, 1)\n",
    "\n",
    "        if tokenizer.padding_side == \"right\":\n",
    "            # Limit positive values to one\n",
    "            p_mask = 1 - p_mask\n",
    "\n",
    "        p_mask[np.where(np.array(span[\"input_ids\"]) == tokenizer.sep_token_id)[0]] = 1\n",
    "\n",
    "        # Set the CLS index to '0'\n",
    "        p_mask[cls_index] = 0\n",
    "\n",
    "        span_is_impossible = example.is_impossible\n",
    "        start_position = 0\n",
    "        end_position = 0\n",
    "        if is_training and not span_is_impossible:\n",
    "            # For training, if our document chunk does not contain an annotation\n",
    "            # we throw it out, since there is nothing to predict.\n",
    "            doc_start = span[\"start\"]\n",
    "            doc_end = span[\"start\"] + span[\"length\"] - 1\n",
    "            out_of_span = False\n",
    "\n",
    "            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):\n",
    "                out_of_span = True\n",
    "\n",
    "            if out_of_span:\n",
    "                start_position = cls_index\n",
    "                end_position = cls_index\n",
    "                span_is_impossible = True\n",
    "            else:\n",
    "                if tokenizer.padding_side == \"left\":\n",
    "                    doc_offset = 0\n",
    "                else:\n",
    "                    doc_offset = len(truncated_query) + sequence_added_tokens\n",
    "\n",
    "                start_position = tok_start_position - doc_start + doc_offset\n",
    "                end_position = tok_end_position - doc_start + doc_offset\n",
    "\n",
    "        features.append(\n",
    "            SquadFeatures(\n",
    "                span[\"input_ids\"],\n",
    "                span[\"attention_mask\"],\n",
    "                span[\"token_type_ids\"],\n",
    "                cls_index,\n",
    "                p_mask.tolist(),\n",
    "                example_index=0,  # Can not set unique_id and example_index here. They will be set after multiple processing.\n",
    "                unique_id=0,\n",
    "                paragraph_len=span[\"paragraph_len\"],\n",
    "                token_is_max_context=span[\"token_is_max_context\"],\n",
    "                tokens=span[\"tokens\"],\n",
    "                token_to_orig_map=span[\"token_to_orig_map\"],\n",
    "                start_position=start_position,\n",
    "                end_position=end_position,\n",
    "                is_impossible=span_is_impossible,\n",
    "                qas_id=example.qas_id,\n",
    "            )\n",
    "        )\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _new_check_is_max_context(doc_spans, cur_span_index, position):\n",
    "    \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
    "    # if len(doc_spans) == 1:\n",
    "    # return True\n",
    "    best_score = None\n",
    "    best_span_index = None\n",
    "    for (span_index, doc_span) in enumerate(doc_spans):\n",
    "        end = doc_span[\"start\"] + doc_span[\"length\"] - 1\n",
    "        if position < doc_span[\"start\"]:\n",
    "            continue\n",
    "        if position > end:\n",
    "            continue\n",
    "        num_left_context = position - doc_span[\"start\"]\n",
    "        num_right_context = end - position\n",
    "        score = min(num_left_context, num_right_context) + 0.01 * doc_span[\"length\"]\n",
    "        if best_score is None or score > best_score:\n",
    "            best_score = score\n",
    "            best_span_index = span_index\n",
    "\n",
    "    return cur_span_index == best_span_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadFeatures(object):\n",
    "    \"\"\"\n",
    "    Single squad example features to be fed to a model.\n",
    "    Those features are model-specific and can be crafted from :class:`~transformers.data.processors.squad.SquadExample`\n",
    "    using the :method:`~transformers.data.processors.squad.squad_convert_examples_to_features` method.\n",
    "    Args:\n",
    "        input_ids: Indices of input sequence tokens in the vocabulary.\n",
    "        attention_mask: Mask to avoid performing attention on padding token indices.\n",
    "        token_type_ids: Segment token indices to indicate first and second portions of the inputs.\n",
    "        cls_index: the index of the CLS token.\n",
    "        p_mask: Mask identifying tokens that can be answers vs. tokens that cannot.\n",
    "            Mask with 1 for tokens than cannot be in the answer and 0 for token that can be in an answer\n",
    "        example_index: the index of the example\n",
    "        unique_id: The unique Feature identifier\n",
    "        paragraph_len: The length of the context\n",
    "        token_is_max_context: List of booleans identifying which tokens have their maximum context in this feature object.\n",
    "            If a token does not have their maximum context in this feature object, it means that another feature object\n",
    "            has more information related to that token and should be prioritized over this feature for that token.\n",
    "        tokens: list of tokens corresponding to the input ids\n",
    "        token_to_orig_map: mapping between the tokens and the original text, needed in order to identify the answer.\n",
    "        start_position: start of the answer token index\n",
    "        end_position: end of the answer token index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        token_type_ids,\n",
    "        cls_index,\n",
    "        p_mask,\n",
    "        example_index,\n",
    "        unique_id,\n",
    "        paragraph_len,\n",
    "        token_is_max_context,\n",
    "        tokens,\n",
    "        token_to_orig_map,\n",
    "        start_position,\n",
    "        end_position,\n",
    "        is_impossible,\n",
    "        qas_id: str = None,\n",
    "    ):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.cls_index = cls_index\n",
    "        self.p_mask = p_mask\n",
    "\n",
    "        self.example_index = example_index\n",
    "        self.unique_id = unique_id\n",
    "        self.paragraph_len = paragraph_len\n",
    "        self.token_is_max_context = token_is_max_context\n",
    "        self.tokens = tokens\n",
    "        self.token_to_orig_map = token_to_orig_map\n",
    "\n",
    "        self.start_position = start_position\n",
    "        self.end_position = end_position\n",
    "        self.is_impossible = is_impossible\n",
    "        self.qas_id = qas_id\n",
    "\n",
    "\n",
    "class SquadResult(object):\n",
    "    \"\"\"\n",
    "    Constructs a SquadResult which can be used to evaluate a model's output on the SQuAD dataset.\n",
    "    Args:\n",
    "        unique_id: The unique identifier corresponding to that example.\n",
    "        start_logits: The logits corresponding to the start of the answer\n",
    "        end_logits: The logits corresponding to the end of the answer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, unique_id, start_logits, end_logits, start_top_index=None, end_top_index=None, cls_logits=None):\n",
    "        self.start_logits = start_logits\n",
    "        self.end_logits = end_logits\n",
    "        self.unique_id = unique_id\n",
    "\n",
    "        if start_top_index:\n",
    "            self.start_top_index = start_top_index\n",
    "            self.end_top_index = end_top_index\n",
    "            self.cls_logits = cls_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = squad_convert_example_to_features(a, max_seq_length=384, doc_stride=128, max_query_length=64, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_word = np.array(a.char_to_word_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,\n",
       "        2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  4,  4,\n",
       "        4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  6,\n",
       "        6,  6,  6,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "        8,  8,  8,  8,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11,\n",
       "       11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13,\n",
       "       14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16,\n",
       "       16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
       "       19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21,\n",
       "       22, 22, 22, 22, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24,\n",
       "       25, 25, 25, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
       "       28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30,\n",
       "       30, 30, 30, 31, 31, 31, 32, 32, 32, 33, 33, 33, 33, 34, 34, 34, 34,\n",
       "       34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 36, 36, 36, 36, 36, 36, 36,\n",
       "       36, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 38, 38, 38, 39, 39, 39,\n",
       "       39, 39, 39, 40, 40, 40, 40, 40, 40, 40, 41, 41, 41, 41, 42, 42, 42,\n",
       "       42, 42, 42, 42, 42, 42, 42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 43,\n",
       "       43, 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 45, 45, 45, 45, 45,\n",
       "       45, 45, 45, 45, 46, 46, 46, 46, 46, 46, 46, 47, 47, 47, 47, 47, 48,\n",
       "       48, 48, 49, 49, 49, 49, 49, 49, 49, 49, 49, 50, 50, 50, 50, 51, 51,\n",
       "       51, 51, 52, 52, 52, 52, 52, 53, 53, 53, 53, 53, 53, 53, 53, 54, 54,\n",
       "       54, 55, 55, 56, 56, 56, 56, 56, 56, 56, 57, 57, 57, 57, 57, 58, 58,\n",
       "       58, 58, 58, 58, 58, 58, 59, 59, 59, 59, 60, 60, 60, 60, 60, 60, 60,\n",
       "       61, 61, 61, 61, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 63, 63, 63,\n",
       "       64, 64, 64, 64, 65, 65, 65, 65, 65, 66, 66, 66, 67, 67, 67, 67, 67,\n",
       "       67, 67, 67, 67, 67, 68, 68, 68, 68, 68, 68, 68, 68, 69, 69, 69, 69,\n",
       "       69, 69, 69, 70, 70, 70, 71, 71, 71, 71, 71, 71, 72, 72, 72, 72, 72,\n",
       "       72, 72, 72, 72, 72, 72, 72, 73, 73, 73, 73, 74, 74, 74, 74, 74, 74,\n",
       "       74, 74, 74, 74, 74, 75, 75, 75, 76, 76, 76, 76, 76, 76, 76, 76, 76,\n",
       "       76, 76, 76, 77, 77, 77, 77, 77, 77, 77, 77, 77, 78, 78, 78, 78, 78,\n",
       "       78, 78, 78, 78, 78, 79, 79, 79, 80, 80, 80, 80, 81, 81, 81, 81, 81,\n",
       "       81, 81, 81, 81, 81])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 1,\n",
       " 2: 1,\n",
       " 3: 1,\n",
       " 4: 1,\n",
       " 5: 1,\n",
       " 6: 2,\n",
       " 7: 2,\n",
       " 8: 2,\n",
       " 9: 2,\n",
       " 10: 3,\n",
       " 11: 4,\n",
       " 12: 5,\n",
       " 13: 6,\n",
       " 14: 7,\n",
       " 15: 8,\n",
       " 16: 9,\n",
       " 17: 10,\n",
       " 18: 11,\n",
       " 19: 12,\n",
       " 20: 12,\n",
       " 21: 12,\n",
       " 22: 13,\n",
       " 23: 14,\n",
       " 24: 15,\n",
       " 25: 16,\n",
       " 26: 17,\n",
       " 27: 18,\n",
       " 28: 19,\n",
       " 29: 20,\n",
       " 30: 21,\n",
       " 31: 22,\n",
       " 32: 22,\n",
       " 33: 23,\n",
       " 34: 24,\n",
       " 35: 25,\n",
       " 36: 25,\n",
       " 37: 26,\n",
       " 38: 27,\n",
       " 39: 28,\n",
       " 40: 29,\n",
       " 41: 30,\n",
       " 42: 31,\n",
       " 43: 32,\n",
       " 44: 33,\n",
       " 45: 34,\n",
       " 46: 35,\n",
       " 47: 36,\n",
       " 48: 37,\n",
       " 49: 38,\n",
       " 50: 39,\n",
       " 51: 40,\n",
       " 52: 40,\n",
       " 53: 41,\n",
       " 54: 41,\n",
       " 55: 41,\n",
       " 56: 42,\n",
       " 57: 42,\n",
       " 58: 42,\n",
       " 59: 42,\n",
       " 60: 42,\n",
       " 61: 43,\n",
       " 62: 44,\n",
       " 63: 44,\n",
       " 64: 45,\n",
       " 65: 45,\n",
       " 66: 45,\n",
       " 67: 46,\n",
       " 68: 46,\n",
       " 69: 46,\n",
       " 70: 47,\n",
       " 71: 48,\n",
       " 72: 49,\n",
       " 73: 50,\n",
       " 74: 51,\n",
       " 75: 52,\n",
       " 76: 53,\n",
       " 77: 53,\n",
       " 78: 53,\n",
       " 79: 54,\n",
       " 80: 55,\n",
       " 81: 56,\n",
       " 82: 57,\n",
       " 83: 58,\n",
       " 84: 59,\n",
       " 85: 60,\n",
       " 86: 61,\n",
       " 87: 62,\n",
       " 88: 63,\n",
       " 89: 64,\n",
       " 90: 65,\n",
       " 91: 66,\n",
       " 92: 67,\n",
       " 93: 68,\n",
       " 94: 69,\n",
       " 95: 70,\n",
       " 96: 71,\n",
       " 97: 72,\n",
       " 98: 73,\n",
       " 99: 74,\n",
       " 100: 75,\n",
       " 101: 75,\n",
       " 102: 75,\n",
       " 103: 76,\n",
       " 104: 77,\n",
       " 105: 78,\n",
       " 106: 78,\n",
       " 107: 79,\n",
       " 108: 80,\n",
       " 109: 81,\n",
       " 110: 81}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0].token_to_orig_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(char_to_word == b[0].token_to_orig_map[0])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(char_to_word == b[0].token_to_orig_map[7])[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WASHINGTON ―Kirk Kerkorian,'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(a.doc_tokens[b[0].token_to_orig_map[0] : b[0].token_to_orig_map[8] + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('dev-v2.0.json' , 'r') as reader:\n",
    "    jf = json.loads(reader.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-6-2d46f4c7ce04>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-2d46f4c7ce04>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    break\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = []\n",
    "def data_append(file, dt):\n",
    "\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
