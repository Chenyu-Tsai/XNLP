{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from transformers.modeling_utils import (WEIGHTS_NAME, PretrainedConfig, PreTrainedModel,\n",
    "                             SequenceSummary, PoolerAnswerClass, PoolerEndLogits, PoolerStartLogits)\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification, XLNetPreTrainedModel, XLNetModel\n",
    "from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from torch.utils.data.dataset import ConcatDataset\n",
    "from XLNet import Dataset_3Way, Dataset_multi, SquadExample, squad_convert_example_to_features\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import re\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Span_Detection(Dataset):\n",
    "    \n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"data/train_span_detection\"]\n",
    "        self.mode = mode\n",
    "        self.df = pd.read_csv(mode + \".tsv\", sep=\"\\t\").fillna(\"\")\n",
    "        self.len = len(self.df)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        context_text, question_text, answer_text, start_position_character = self.df.iloc[idx,:].values\n",
    "        \n",
    "        example = SquadExample(\n",
    "            question_text=question_text,\n",
    "            context_text=context_text,\n",
    "            answer_text=answer_text,\n",
    "            start_position_character=start_position_character\n",
    "        )\n",
    "\n",
    "        features = squad_convert_example_to_features(example,\n",
    "                                                     max_seq_length=384,\n",
    "                                                     doc_stride=128,\n",
    "                                                     max_query_length=128\n",
    "                                                    )\n",
    "        input_ids = torch.tensor(features[0].input_ids)\n",
    "        attention_mask = torch.tensor(features[0].attention_mask)\n",
    "        token_type_ids = torch.tensor(features[0].token_type_ids)\n",
    "        start_position = torch.tensor(features[0].start_position)\n",
    "        end_position = torch.tensor(features[0].end_position)\n",
    "        cls_index = torch.tensor(features[0].cls_index)\n",
    "        p_mask = torch.tensor(features[0].p_mask)\n",
    "        \n",
    "        return input_ids, attention_mask, token_type_ids, start_position, end_position, cls_index, p_mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)\n",
    "\n",
    "trainset_3way = Dataset_3Way(\"data/RTE5_train\", tokenizer=tokenizer)\n",
    "trainset_multi = Dataset_multi(\"data/train_multi_label\", tokenizer = tokenizer)\n",
    "#trainset_span = Dataset_Span_Detection(\"data/train_span_detection\", tokenizer=tokenizer)\n",
    "#trainset = ConcatDataset([trainset_3way, trainset_multi, trainset_span])\n",
    "trainset = ConcatDataset([trainset_3way, trainset_multi])\n",
    "#trainset = Dataset_3Way(\"data/RTE5_train\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2011,    47,    72,  1390,    29,  1220,  3469,    25,    18,  7765,\n",
       "             28,  2279, 24512,   813,  2349, 23397,   780,   431, 15261,    41,\n",
       "             50,   163,  1060,    76,  1068,   456,     9,    36,   909,    99,\n",
       "           1645,   891,    25,    17,  3654,   577,  8917,   365,    99,    24,\n",
       "          13456,    25,    18,  1808,  2182,  7759,    90,   442,   108,    18,\n",
       "          10733,    30,   194,  2279,     9,    18,  2510,  1525,    70,   100,\n",
       "            139,   754,   526,    21,   891,    42,  9875,    23,    54,    50,\n",
       "            655,    72,    25,  2282,     9,   365,    42,  3469,    55,   163,\n",
       "          13782,    25,    24,  2175,  4140,    21, 14107,   162,     9,     4,\n",
       "           2349, 23397,   780,   431, 15261, 19709,    25,    17,  3654,   577,\n",
       "              9,     4,     3]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 2]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1]]),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    masks_tensors = [s[2] for s in samples]\n",
    "    if samples[0][3] is not None:\n",
    "        label_ids = torch.stack([s[3] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "        \n",
    "    # zero pad 到同一序列長度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                  batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, \n",
    "                                    batch_first=True)\n",
    "    masks_tensors = pad_sequence(masks_tensors, \n",
    "                                    batch_first=True)\n",
    "\n",
    "    return tokens_tensors.squeeze(1), segments_tensors.squeeze(1), masks_tensors.squeeze(1), label_ids\n",
    "\n",
    "\n",
    "# 初始化回傳訓練樣本的 DataLoader\n",
    "# 利用 `collate_fn` 將 list of samples 合併成一個 mini-batch \n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=1,collate_fn=create_mini_batch, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLNetForMultiSequenceClassification(XLNetPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = 3\n",
    "        self.num_labels_3way = 3\n",
    "        self.num_labels_multi = 5\n",
    "        \n",
    "        self.transformer = XLNetModel(config)\n",
    "        self.sequence_summary = SequenceSummary(config)\n",
    "        self.logits_proj_3way = nn.Linear(config.d_model, self.num_labels_3way)\n",
    "        self.logits_proj_multi = nn.Linear(config.d_model, self.num_labels_multi)\n",
    "        self.weights_3way = [0.3, 0.5, 2]\n",
    "        #self.weights_multi = [15, 10, 15, 5, 5]\n",
    "        self.weights_multi = [6, 4, 6, 2, 2]\n",
    "        self.class_weights_3way = torch.FloatTensor(self.weights_3way).cuda()\n",
    "        self.class_weights_multi = torch.FloatTensor(self.weights_multi).cuda()\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, mems=None, perm_mask=None, target_mapping=None,\n",
    "                token_type_ids=None, input_mask=None, head_mask=None, labels=None, inputs_embeds=None):\n",
    "        transformer_outputs = self.transformer(input_ids,\n",
    "                                               attention_mask=attention_mask,\n",
    "                                               mems=mems,\n",
    "                                               perm_mask=perm_mask,\n",
    "                                               target_mapping=target_mapping,\n",
    "                                               token_type_ids=token_type_ids,\n",
    "                                               input_mask=input_mask, \n",
    "                                               head_mask=head_mask,\n",
    "                                               inputs_embeds=inputs_embeds)\n",
    "    \n",
    "        output = transformer_outputs[0]\n",
    "        output = self.sequence_summary(output)\n",
    "        \n",
    "        if labels is None:\n",
    "            logits = self.logits_proj_3way(output)\n",
    "            outputs = (logits,) + transformer_outputs[1:]\n",
    "\n",
    "        if labels is not None:\n",
    "            task_check = 0\n",
    "            if labels.size() == torch.Size([1]):\n",
    "                logits_3way = self.logits_proj_3way(output)\n",
    "                outputs = (logits_3way,) + transformer_outputs[1:]\n",
    "                task_check = 1\n",
    "            else:\n",
    "                logits_multi = self.logits_proj_multi(output)\n",
    "                outputs = (logits_multi,) + transformer_outputs[1:]\n",
    "\n",
    "            if task_check:\n",
    "                loss_fct = CrossEntropyLoss(weight=self.class_weights_3way)\n",
    "                loss = loss_fct(logits_3way.view(-1, self.num_labels_3way), labels.view(-1)).cuda()\n",
    "            else:\n",
    "                loss_fct = BCEWithLogitsLoss(pos_weight=self.class_weights_multi)\n",
    "                loss = loss_fct(logits_multi.view(-1, self.num_labels_multi), labels).cuda()\n",
    "            outputs = (loss,) + outputs\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"xlnet-base-cased\"\n",
    "model = XLNetForMultiSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME,\n",
    "                                                            output_attentions=True,\n",
    "                                                            dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for data in dataloader:\n",
    "            \n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "\n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a949011c8b945339e25a5c68cc0be2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=20, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d94cf822cca4b399f6373f6772ac2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=718, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([])) must be the same as input size (torch.Size([1, 5]))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-5c209f533ecc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'EPOCHS = 20\\nbatch_size = 4\\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=EPOCHS * ((len(trainloader)//batch_size)))\\nepochs_trained = 0\\n\\nmodel.zero_grad()\\ntrain_iterator = trange(epochs_trained, EPOCHS, desc=\"Epoch\")\\nset_seed(42)\\n\\nfor _ in train_iterator:\\n    epoch_iterator = tqdm(trainset, desc=\"Iteration\")\\n    \\n    model.train()\\n    running_loss = 0.0\\n    batch_cnt = 1\\n    loss = torch.zeros(1).to(device)\\n    \\n    for step, data in enumerate(epoch_iterator):\\n        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\\n        \\n        # forward pass\\n        outputs = model(input_ids=tokens_tensors, \\n                        token_type_ids=segments_tensors, \\n                        attention_mask=masks_tensors, \\n                        labels=labels)\\n        batch_cnt += 1\\n        loss = outputs[0]/batch_size\\n        loss.backward()\\n        \\n        if batch_cnt >= batch_size:\\n            optimizer.step()\\n            scheduler.step()\\n            model.zero_grad()\\n            batch_cnt = 0\\n\\n        # 紀錄當前 batch loss\\n        running_loss += loss.item()\\n    epochs_trained += 1\\n        \\n    testset = Dataset_3Way(\"data/RTE5_test\", tokenizer=tokenizer)\\n    testloader = DataLoader(testset, batch_size=1, \\n                     collate_fn=create_mini_batch)\\n    predictions = get_predictions(model, testloader)\\n\\n    df_pred = pd.DataFrame({\"label\": predictions.tolist()})\\n        \\n    pred_Y = df_pred[\\'label\\'].values\\n    test_Y = pd.read_csv(\"data/RTE5_test.tsv\", sep=\\'\\\\t\\').fillna(\"\")[\\'label\\'].values\\n\\n    accuracy = accuracy_score(test_Y, np.array(pred_Y))\\n    precision = precision_score(test_Y, pred_Y, average=\\'macro\\')\\n    recall = recall_score(test_Y, pred_Y, average=\\'macro\\')\\n    fscore = f1_score(test_Y, pred_Y, average=\\'macro\\')\\n    \\n    CNT = 0\\n    TOTAL = 0\\n    for i in range(len(test_Y)):\\n        if test_Y[i] == 2:\\n            TOTAL += 1\\n        else:\\n            pass\\n        if test_Y[i] == 2 and predictions[i] == 2:\\n            CNT += 1\\n    contra = round((CNT/TOTAL)*100,1)\\n    if contra > 20 and accuracy > 0.6:\\n        torch.save(model, \"multi_%g, %g.pkl\" % accuracy, CNT)\\n    print(\"Accuracy: %g\\\\tPrecision: %g\\\\tRecall: %g\\\\tF-score: %g Loss: %g\" % (accuracy, precision, recall, fscore, running_loss))\\n    print(contra)\\n    print(\"------------------------------------------\")'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2116\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2117\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2118\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<C:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\decorator.py:decorator-gen-61>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1191\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1193\u001b[1;33m             \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-63-70a96b28678a>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, labels, inputs_embeds)\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weights_multi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits_multi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_labels_multi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    599\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m                                                   reduction=self.reduction)\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   2110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2111\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2112\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2114\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Target size (torch.Size([])) must be the same as input size (torch.Size([1, 5]))"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EPOCHS = 20\n",
    "batch_size = 4\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=EPOCHS * ((len(trainloader)//batch_size)))\n",
    "epochs_trained = 0\n",
    "\n",
    "model.zero_grad()\n",
    "train_iterator = trange(epochs_trained, EPOCHS, desc=\"Epoch\")\n",
    "set_seed(42)\n",
    "\n",
    "for _ in train_iterator:\n",
    "    epoch_iterator = tqdm(trainset, desc=\"Iteration\")\n",
    "    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    batch_cnt = 1\n",
    "    loss = torch.zeros(1).to(device)\n",
    "    \n",
    "    for step, data in enumerate(epoch_iterator):\n",
    "        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "        batch_cnt += 1\n",
    "        loss = outputs[0]/batch_size\n",
    "        loss.backward()\n",
    "        \n",
    "        if batch_cnt >= batch_size:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "            batch_cnt = 0\n",
    "\n",
    "        # 紀錄當前 batch loss\n",
    "        running_loss += loss.item()\n",
    "    epochs_trained += 1\n",
    "        \n",
    "    testset = Dataset_3Way(\"data/RTE5_test\", tokenizer=tokenizer)\n",
    "    testloader = DataLoader(testset, batch_size=1, \n",
    "                     collate_fn=create_mini_batch)\n",
    "    predictions = get_predictions(model, testloader)\n",
    "\n",
    "    df_pred = pd.DataFrame({\"label\": predictions.tolist()})\n",
    "        \n",
    "    pred_Y = df_pred['label'].values\n",
    "    test_Y = pd.read_csv(\"data/RTE5_test.tsv\", sep='\\t').fillna(\"\")['label'].values\n",
    "\n",
    "    accuracy = accuracy_score(test_Y, np.array(pred_Y))\n",
    "    precision = precision_score(test_Y, pred_Y, average='macro')\n",
    "    recall = recall_score(test_Y, pred_Y, average='macro')\n",
    "    fscore = f1_score(test_Y, pred_Y, average='macro')\n",
    "    \n",
    "    CNT = 0\n",
    "    TOTAL = 0\n",
    "    for i in range(len(test_Y)):\n",
    "        if test_Y[i] == 2:\n",
    "            TOTAL += 1\n",
    "        else:\n",
    "            pass\n",
    "        if test_Y[i] == 2 and predictions[i] == 2:\n",
    "            CNT += 1\n",
    "    contra = round((CNT/TOTAL)*100,1)\n",
    "    if contra > 20 and accuracy > 0.6:\n",
    "        torch.save(model, \"multi_%g, %g.pkl\" % accuracy, CNT)\n",
    "    print(\"Accuracy: %g\\tPrecision: %g\\tRecall: %g\\tF-score: %g Loss: %g\" % (accuracy, precision, recall, fscore, running_loss))\n",
    "    print(contra)\n",
    "    print(\"------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a93b3cad9d045b199fcedc5a04adbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=20, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcece11477634248a4a66cacc60e2a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=500, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "dimension mismatch for operand 0: equation 3 tensor 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-f9ce0c75fa6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'EPOCHS = 20\\nbatch_size = 4\\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=EPOCHS * ((len(trainloader)//batch_size)))\\nepochs_trained = 0\\n\\nmodel.zero_grad()\\ntrain_iterator = trange(epochs_trained, EPOCHS, desc=\"Epoch\")\\nset_seed(42)\\n\\nfor _ in train_iterator:\\n    epoch_iterator = tqdm(trainloader, desc=\"Iteration\")\\n    \\n    model.train()\\n    running_loss = 0.0\\n    batch_cnt = 1\\n    loss = torch.zeros(1).to(device)\\n    \\n    for step, data in enumerate(epoch_iterator):\\n        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\\n        \\n        # forward pass\\n        outputs = model(input_ids=tokens_tensors, \\n                        token_type_ids=segments_tensors, \\n                        attention_mask=masks_tensors, \\n                        labels=labels)\\n        batch_cnt += 1\\n        loss = outputs[0]/batch_size\\n        loss.backward()\\n        \\n        if batch_cnt >= batch_size:\\n            optimizer.step()\\n            scheduler.step()\\n            model.zero_grad()\\n            batch_cnt = 0\\n\\n        # 紀錄當前 batch loss\\n        running_loss += loss.item()\\n    epochs_trained += 1\\n        \\n    testset = Dataset_3Way(\"data/RTE5_test\", tokenizer=tokenizer)\\n    testloader = DataLoader(testset, batch_size=1, \\n                     collate_fn=create_mini_batch)\\n    predictions = get_predictions(model, testloader)\\n\\n    df_pred = pd.DataFrame({\"label\": predictions.tolist()})\\n        \\n    pred_Y = df_pred[\\'label\\'].values\\n    test_Y = pd.read_csv(\"data/RTE5_test.tsv\", sep=\\'\\\\t\\').fillna(\"\")[\\'label\\'].values\\n\\n    accuracy = accuracy_score(test_Y, np.array(pred_Y))\\n    precision = precision_score(test_Y, pred_Y, average=\\'macro\\')\\n    recall = recall_score(test_Y, pred_Y, average=\\'macro\\')\\n    fscore = f1_score(test_Y, pred_Y, average=\\'macro\\')\\n    \\n    CNT = 0\\n    TOTAL = 0\\n    for i in range(len(test_Y)):\\n        if test_Y[i] == 2:\\n            TOTAL += 1\\n        else:\\n            pass\\n        if test_Y[i] == 2 and predictions[i] == 2:\\n            CNT += 1\\n    contra = round((CNT/TOTAL)*100,1)\\n    if contra > 20 and accuracy > 0.6:\\n        torch.save(model, \"multi_%g, %g.pkl\" % accuracy, CNT)\\n    print(\"Accuracy: %g\\\\tPrecision: %g\\\\tRecall: %g\\\\tF-score: %g Loss: %g\" % (accuracy, precision, recall, fscore, running_loss))\\n    print(contra)\\n    print(\"------------------------------------------\")\\n        '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2116\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2117\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2118\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<C:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\decorator.py:decorator-gen-61>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1191\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1193\u001b[1;33m             \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-a4977d1b39d8>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, labels, inputs_embeds)\u001b[0m\n\u001b[0;32m     30\u001b[0m                                                \u001b[0minput_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                                                \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                                                inputs_embeds=inputs_embeds)\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\transformers\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds)\u001b[0m\n\u001b[0;32m    880\u001b[0m                 \u001b[0mmems\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmems\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m                 \u001b[0mtarget_mapping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_mapping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 882\u001b[1;33m                 \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    883\u001b[0m             )\n\u001b[0;32m    884\u001b[0m             \u001b[0moutput_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_g\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\transformers\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, output_h, output_g, attn_mask_h, attn_mask_g, r, seg_mat, mems, target_mapping, head_mask)\u001b[0m\n\u001b[0;32m    442\u001b[0m             \u001b[0mmems\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmems\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[0mtarget_mapping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_mapping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    445\u001b[0m         )\n\u001b[0;32m    446\u001b[0m         \u001b[0moutput_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_g\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\transformers\\modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, h, g, attn_mask_h, attn_mask_g, r, seg_mat, mems, target_mapping, head_mask)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m             \u001b[1;31m# content heads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m             \u001b[0mq_head_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ibh,hnd->ibnd\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m             \u001b[0mk_head_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ibh,hnd->ibnd\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m             \u001b[0mv_head_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ibh,hnd->ibnd\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36meinsum\u001b[1;34m(equation, *operands)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[1;31m# the old interface of passing the operands as one list argument\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[0moperands\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moperands\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: dimension mismatch for operand 0: equation 3 tensor 4"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EPOCHS = 20\n",
    "batch_size = 4\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=EPOCHS * ((len(trainloader)//batch_size)))\n",
    "epochs_trained = 0\n",
    "\n",
    "model.zero_grad()\n",
    "train_iterator = trange(epochs_trained, EPOCHS, desc=\"Epoch\")\n",
    "set_seed(42)\n",
    "\n",
    "for _ in train_iterator:\n",
    "    epoch_iterator = tqdm(trainloader, desc=\"Iteration\")\n",
    "    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    batch_cnt = 1\n",
    "    loss = torch.zeros(1).to(device)\n",
    "    \n",
    "    for step, data in enumerate(epoch_iterator):\n",
    "        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "        batch_cnt += 1\n",
    "        loss = outputs[0]/batch_size\n",
    "        loss.backward()\n",
    "        \n",
    "        if batch_cnt >= batch_size:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "            batch_cnt = 0\n",
    "\n",
    "        # 紀錄當前 batch loss\n",
    "        running_loss += loss.item()\n",
    "    epochs_trained += 1\n",
    "        \n",
    "    testset = Dataset_3Way(\"data/RTE5_test\", tokenizer=tokenizer)\n",
    "    testloader = DataLoader(testset, batch_size=1, \n",
    "                     collate_fn=create_mini_batch)\n",
    "    predictions = get_predictions(model, testloader)\n",
    "\n",
    "    df_pred = pd.DataFrame({\"label\": predictions.tolist()})\n",
    "        \n",
    "    pred_Y = df_pred['label'].values\n",
    "    test_Y = pd.read_csv(\"data/RTE5_test.tsv\", sep='\\t').fillna(\"\")['label'].values\n",
    "\n",
    "    accuracy = accuracy_score(test_Y, np.array(pred_Y))\n",
    "    precision = precision_score(test_Y, pred_Y, average='macro')\n",
    "    recall = recall_score(test_Y, pred_Y, average='macro')\n",
    "    fscore = f1_score(test_Y, pred_Y, average='macro')\n",
    "    \n",
    "    CNT = 0\n",
    "    TOTAL = 0\n",
    "    for i in range(len(test_Y)):\n",
    "        if test_Y[i] == 2:\n",
    "            TOTAL += 1\n",
    "        else:\n",
    "            pass\n",
    "        if test_Y[i] == 2 and predictions[i] == 2:\n",
    "            CNT += 1\n",
    "    contra = round((CNT/TOTAL)*100,1)\n",
    "    if contra > 20 and accuracy > 0.6:\n",
    "        torch.save(model, \"multi_%g, %g.pkl\" % accuracy, CNT)\n",
    "    print(\"Accuracy: %g\\tPrecision: %g\\tRecall: %g\\tF-score: %g Loss: %g\" % (accuracy, precision, recall, fscore, running_loss))\n",
    "    print(contra)\n",
    "    print(\"------------------------------------------\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type XLNetForMultiSequenceClassification. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type XLNetModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type XLNetLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type XLNetRelativeAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type XLNetFeedForward. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type SequenceSummary. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\cheny\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Identity. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, \"multi_%g, %g.pkl\" % (accuracy, CNT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = get_predictions(model, dataloader_3way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 42.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 測試集\n",
    "testset = Dataset_3Way(\"test\", tokenizer=tokenizer)\n",
    "testloader = DataLoader(testset, batch_size=1, \n",
    "                        collate_fn=create_mini_batch)\n",
    "\n",
    "predictions = get_predictions(model, testloader)\n",
    "\n",
    "# 將預測的 label id 轉回文字\n",
    "index_map = {v: k for k, v in testset.label_map.items()}\n",
    "\n",
    "df_pred = pd.DataFrame({\"label\": predictions.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.575\tPrecision: 0.492086\tRecall: 0.489153\tF-score: 0.482597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "pred_Y = df_pred['label'].values\n",
    "test_Y = pd.read_csv('test.tsv', sep='\\t').fillna(\"\")['label'].values\n",
    "\n",
    "accuracy = accuracy_score(test_Y, np.array(pred_Y))\n",
    "precision = precision_score(test_Y, pred_Y, average='macro')\n",
    "recall = recall_score(test_Y, pred_Y, average='macro')\n",
    "fscore = f1_score(test_Y, pred_Y, average='macro')\n",
    "\n",
    "print(\"Accuracy: %g\\tPrecision: %g\\tRecall: %g\\tF-score: %g\" % (\n",
    "    accuracy, precision, recall, fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-18c85840c052>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'contra_61_28.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(model, 'contra_61_28.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = torch.load('test.pkl',map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'gelu' on <module 'transformers.modeling_xlnet' from '/Users/chenyutsai/opt/anaconda3/lib/python3.7/site-packages/transformers/modeling_xlnet.py'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-e2ee2fec08c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#model_multi = model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_pretrained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXLNetModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel_single\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'acc_0.52_complete.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXLNetTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xlnet-base-cased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'encoding'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'gelu' on <module 'transformers.modeling_xlnet' from '/Users/chenyutsai/opt/anaconda3/lib/python3.7/site-packages/transformers/modeling_xlnet.py'>"
     ]
    }
   ],
   "source": [
    "from bertviz import bertviz\n",
    "\n",
    "model_version = 'xlnet-base-cased'\n",
    "#model_multi = model \n",
    "model_pretrained = XLNetModel.from_pretrained(model_version, output_attentions=True)\n",
    "model_single = torch.load('acc_0.52_complete.pkl',map_location=torch.device('cpu'))\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explainability_compare(model, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b):\n",
    "    inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt', add_special_tokens=True)\n",
    "    input_ids = inputs['input_ids'].cuda()\n",
    "    input_ids.squeeze()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "    token_type_ids = inputs['token_type_ids'].cuda()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, token_type_ids=token_type_ids)[1]\n",
    "    attention = model(input_ids, token_type_ids=token_type_ids)[-1]\n",
    "    \n",
    "    attn = format_attention(attention, tokens)  \n",
    "    tokens = format_special_chars(tokens)\n",
    "    sentence_b_start = token_type_ids[0].tolist().index(1)\n",
    "    slice_a = slice(0, sentence_b_start)\n",
    "    slice_b = slice(sentence_b_start, len(tokens))\n",
    "    attn_data = attn[:, :, slice_a, slice_b]\n",
    "    sentence_a_tokens = tokens[slice_a]\n",
    "    sentence_b_tokens = tokens[slice_b]\n",
    "    pair = pair_match(sentence_a_tokens, sentence_b_tokens, attn_data=attn_data)\n",
    "    pair = sorted(pair, key=lambda pair: pair[2], reverse=True)\n",
    "    pair = pair_return(pair)\n",
    "    \n",
    "    test_inputs = tokenizer.encode_plus(test_sentence_a, test_sentence_b, return_tensors='pt', add_special_tokens=False)\n",
    "    test_input_ids = test_inputs['input_ids']\n",
    "    test_input_ids.squeeze()\n",
    "    test_tokens = tokenizer.convert_ids_to_tokens(test_input_ids.squeeze().tolist())\n",
    "    test_token_type_ids = test_inputs['token_type_ids']\n",
    "    test_tokens = format_special_chars(test_tokens)\n",
    "    test_sentence_b_start = test_token_type_ids[0].tolist().index(1)\n",
    "    test_slice_a = slice(0, test_sentence_b_start)\n",
    "    test_slice_b = slice(test_sentence_b_start, len(test_tokens))\n",
    "    test_sentence_a_tokens = test_tokens[test_slice_a]\n",
    "    test_sentence_b_tokens = test_tokens[test_slice_b]\n",
    "    test_pair = pair_match(test_sentence_a_tokens, test_sentence_b_tokens, attn_data=None)\n",
    "\n",
    "    return MRR_calculate(test_pair, pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt', add_special_tokens=False)\n",
    "input_ids = inputs['input_ids']\n",
    "input_ids.squeeze()\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "token_type_ids = inputs['token_type_ids']\n",
    "tokens = format_special_chars(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_a = 'sentence 1'\n",
    "# sentence_b = 'sentence 2'\n",
    "sentence_a = \"\"\"a soyuz capsule carrying a russian cosmonaut, an american astronaut and u.s. billionaire tourist charles simonyi has docked at the international space station. russian cosmonaut gennady padalka manually guided the capsule to a stop ahead of schedule saturday two days after blasting off from the baikonur cosmodrome in kazakhstan. the crews of the capsule and the station will spend around three hours checking seals before opening the air locks and meeting up facetoface.\n",
    "\"\"\"\n",
    "sentence_b = \"charles simonyi is a russian cosmonaut.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = Dataset_3Way(\"test_2\", tokenizer=tokenizer)\n",
    "testloader = DataLoader(testset, batch_size=1, collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5723, 0.1841, 0.1501]])\n",
      "tensor([[ 0.7128, -1.1001, -0.2352]])\n",
      "tensor([[ 1.5935, -1.3127, -0.5411]])\n"
     ]
    }
   ],
   "source": [
    "predictions = get_predictions(model2, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1305,  0.1911, -0.3580]])\n",
      "tensor([[ 0.5888, -0.6714, -0.3910]])\n",
      "tensor([[ 0.8907, -0.8635, -0.5242]])\n"
     ]
    }
   ],
   "source": [
    "testset = Dataset_3Way(\"test_2\", tokenizer=tokenizer)\n",
    "testloader = DataLoader(testset, batch_size=1, collate_fn=create_mini_batch)\n",
    "predictions = get_predictions(model2, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.3032, -1.0553, -0.4880]])\n"
     ]
    }
   ],
   "source": [
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model2(input_ids, token_type_ids=token_type_ids)[0]\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt', add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = inputs['input_ids'].cuda()\n",
    "input_ids.squeeze()\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "token_type_ids = inputs['token_type_ids'].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model_trained(input_ids, token_type_ids=token_type_ids)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_trained = model_trained(input_ids, token_type_ids=token_type_ids)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = inputs['input_ids']\n",
    "input_ids.squeeze()\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "token_type_ids = inputs['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = model(input_ids, token_type_ids=token_type_ids)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "input_id_list\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_id_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_attention(attention, tokens):\n",
    "    for i, t in enumerate(tokens):\n",
    "        if t in (\"<sep>\", \"<cls>\"):\n",
    "            for layer_attn in attention:\n",
    "                layer_attn[0, :, i, :] = 0\n",
    "                layer_attn[0, :, :, i] = 0\n",
    "    squeezed = []\n",
    "    for layer_attention in attention:\n",
    "        # 1 x num_heads x seq_len x seq_len\n",
    "        squeezed.append(layer_attention.squeeze(0))\n",
    "    # num_layers x num_heads x seq_len x seq_len\n",
    "    return torch.stack(squeezed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_special_chars(tokens):\n",
    "    return [t.replace('Ġ', '').replace('▁', '').replace('</w>', '') for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_score(attn_data, index_a, index_b):\n",
    "    score = 0.\n",
    "    for layer in attn_data:\n",
    "        for head in layer:\n",
    "            score_individaul = head[index_a][index_b].tolist()\n",
    "            score += score_individaul\n",
    "    return round(score, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_match(sentence_a_tokens, sentence_b_tokens, attn_data=None):\n",
    "    whole = []\n",
    "    for token_a in sentence_a_tokens:\n",
    "        index_a = sentence_a_tokens.index(token_a)\n",
    "        for token_b in sentence_b_tokens:\n",
    "            index_b = sentence_b_tokens.index(token_b)\n",
    "            if attn_data is not None:\n",
    "                score = look_score(attn_data, index_a, index_b)\n",
    "                pair = (token_a, token_b, score)\n",
    "                if score != 0:\n",
    "                    whole.append(pair)\n",
    "            else:\n",
    "                pair = (token_a, token_b)\n",
    "                whole.append(pair)\n",
    "    return whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = format_attention(attention, tokens)\n",
    "attn_trained = format_attention(attention_trained, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = format_special_chars(tokens)\n",
    "sentence_b_start = token_type_ids[0].tolist().index(1)\n",
    "slice_a = slice(0, sentence_b_start)\n",
    "slice_b = slice(sentence_b_start, len(tokens))\n",
    "attn_data = attn[:, :, slice_a, slice_b]\n",
    "attn_data_trained = attn_trained[:, :, slice_a, slice_b]\n",
    "sentence_a_tokens = tokens[slice_a]\n",
    "sentence_b_tokens = tokens[slice_b]\n",
    "pair = pair_match(sentence_a_tokens, sentence_b_tokens, attn_data=attn_data)\n",
    "pair = sorted(pair, key=lambda pair: pair[2], reverse=True) \n",
    "pair_trained = pair_match(sentence_a_tokens, sentence_b_tokens, attn_data_trained)\n",
    "pair_trained = sorted(pair_trained, key=lambda pair_trained: pair_trained[2], reverse=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_return(pair):\n",
    "    lst = []\n",
    "    for a, b, s in pair:\n",
    "        p = (a, b)\n",
    "        lst.append(p)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = pair_return(pair)\n",
    "pair_trained = pair_return(pair_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'a'), ('A', 'a'), ('game', 'sport'), ('soccer', 'sport')]"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sampling = random.choices(pair_truth, k=4)\n",
    "sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence_a = \"\"\"an american astronaut and u.s. billionaire tourist charles simonyi\"\"\"\n",
    "test_sentence_b = 'charles simonyi is a russian cosmonaut'\n",
    "\n",
    "inputs = tokenizer.encode_plus(test_sentence_a, test_sentence_b, return_tensors='pt', add_special_tokens=False)\n",
    "input_ids = inputs['input_ids']\n",
    "input_ids.squeeze()\n",
    "test_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
    "test_token_type_ids = inputs['token_type_ids']\n",
    "test_tokens = format_special_chars(test_tokens)\n",
    "test_sentence_b_start = test_token_type_ids[0].tolist().index(1)\n",
    "slice_a = slice(0, test_sentence_b_start)\n",
    "slice_b = slice(test_sentence_b_start, len(test_tokens))\n",
    "test_sentence_a_tokens = test_tokens[slice_a]\n",
    "test_sentence_b_tokens = test_tokens[slice_b]\n",
    "test_pair = pair_match(test_sentence_a_tokens, test_sentence_b_tokens, attn_data=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MRR_calculate(pair_truth, pair_all):\n",
    "    final_score = 0.\n",
    "    for query in pair_truth:\n",
    "        for response in range(len(pair_all)):\n",
    "            if pair_all[response] == query:\n",
    "                score = 1/(response+1)\n",
    "                final_score += score\n",
    "    final_score = final_score/len(pair_truth)\n",
    "    return final_score\n",
    "\n",
    "def MRR_mean(pair_truth, pair_all, top_k, times):\n",
    "    filtered = random.choices(pair_truth, k=top_k)\n",
    "    final = 0.\n",
    "    for i in range(times):\n",
    "        score = MRR_calculate(filtered, pair_all)\n",
    "        final += score\n",
    "    final = final/times\n",
    "    return final\n",
    "\n",
    "# first = MRR_mean(test_pair, pair, 6, 1000)\n",
    "# second = MRR_mean(test_pair, pair_trained, 6, 1000)\n",
    "# print(first, second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contradiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained: 0.024615\n",
      "SingleTask: 0.015987\n",
      "MultiTask: 0.025282\n"
     ]
    }
   ],
   "source": [
    "sentence_a = \"\"\"a soyuz capsule carrying a russian cosmonaut, an american astronaut and u.s. billionaire tourist charles simonyi has docked at the international space station. russian cosmonaut gennady padalka manually guided the capsule to a stop ahead of schedule saturday two days after blasting off from the baikonur cosmodrome in kazakhstan. the crews of the capsule and the station will spend around three hours checking seals before opening the air locks and meeting up facetoface.\n",
    "\"\"\"\n",
    "sentence_b = \"charles simonyi is a russian cosmonaut.\"\n",
    "\n",
    "test_sentence_a = \"an american astronaut and u.s. billionaire tourist charles simonyi\"\n",
    "test_sentence_b = 'charles simonyi is a russian cosmonaut'\n",
    "\n",
    "model_pretrained.to(device)\n",
    "X_pretrained = explainability_compare(model_pretrained, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_single = explainability_compare(model_single, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_multi = explainability_compare(model_multi, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "\n",
    "print(\"Pretrained: %6f\\nSingleTask: %6f\\nMultiTask: %6f\" % (X_pretrained, X_single, X_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained: 0.011633\n",
      "SingleTask: 0.012457\n",
      "MultiTask: 0.019987\n"
     ]
    }
   ],
   "source": [
    "sentence_a = \"\"\"bout who is generally believed to be a model for the arms dealer portrayed by nicolas cage in the 2005 movie 'lord of war'  has repeatedly denied any involvement in illicit activities. at a hearing earlier this month, he angrily accused the united states of framing him and pressuring thailand to extradite him. he has long been linked to some of the world's most notorious conflicts, allegedly supplying arms to former liberian dictator charles taylor and libyan leader colonel gaddafi.\n",
    "\"\"\"\n",
    "sentence_b = \"gaddafi is the liberian dictator.\"\n",
    "\n",
    "test_sentence_a = \"former liberian dictator charles taylor\"\n",
    "test_sentence_b = 'gaddafi is the liberian dictator'\n",
    "\n",
    "model_pretrained.to(device)\n",
    "X_pretrained = explainability_compare(model_pretrained, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_single = explainability_compare(model_single, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_multi = explainability_compare(model_multi, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "\n",
    "print(\"Pretrained: %6f\\nSingleTask: %6f\\nMultiTask: %6f\" % (X_pretrained, X_single, X_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained: 0.022674\n",
      "SingleTask: 0.022773\n",
      "MultiTask: 0.024005\n"
     ]
    }
   ],
   "source": [
    "sentence_a = \"\"\"a man has hijacked a passenger plane in the jamaican resort of montego bay, and is still holding five crew members hostage, reports say. more than 150 passengers were on board when the man forced his way on board the canadabound charter plane but all have now been released. the man breached security and boarded as the plane was about to take off at around 2230 local time 0330 gmt. negotiations for the release of the hostages are said to be taking place.\"\"\"\n",
    "sentence_b = \"a plane crashed in the jamaican resort of montego bay.\"\n",
    "\n",
    "test_sentence_a = \"a man has hijacked a passenger plane in the jamaican resort of montego bay\"\n",
    "test_sentence_b = 'a plane crashed in the jamaican resort of montego bay'\n",
    "\n",
    "model_pretrained.to(device)\n",
    "X_pretrained = explainability_compare(model_pretrained, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_single = explainability_compare(model_single, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_multi = explainability_compare(model_multi, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "\n",
    "print(\"Pretrained: %6f\\nSingleTask: %6f\\nMultiTask: %6f\" % (X_pretrained, X_single, X_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained: 0.017245\n",
      "SingleTask: 0.015898\n",
      "MultiTask: 0.017563\n"
     ]
    }
   ],
   "source": [
    "sentence_a = \"\"\"the hurricane caused severe destruction across the entire mississippi coast and into alabama, as far as 100 miles 160 km from the storm's center. katrina was the eleventh tropical storm, fifth hurricane, third major hurricane, and second category 5 hurricane of the 2005 atlantic season. it formed over the bahamas on august 23, 2005, and crossed southern florida as a moderate category 1 hurricane, causing some deaths and flooding there, before strengthening rapidly in the gulf of mexico and becoming one of the strongest hurricanes on record while at sea.\n",
    "\"\"\"\n",
    "sentence_b = \"hurricane katrina formed in august 2005 in the gulf of mexico.\"\n",
    "\n",
    "test_sentence_a = \"it formed over the bahamas on august 23, 2005\"\n",
    "test_sentence_b = 'hurricane katrina formed in august 2005 in the gulf of mexico.'\n",
    "\n",
    "model_pretrained.to(device)\n",
    "X_pretrained = explainability_compare(model_pretrained, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_single = explainability_compare(model_single, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_multi = explainability_compare(model_multi, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "\n",
    "print(\"Pretrained: %6f\\nSingleTask: %6f\\nMultiTask: %6f\" % (X_pretrained, X_single, X_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained: 0.031021\n",
      "SingleTask: 0.027749\n",
      "MultiTask: 0.033714\n"
     ]
    }
   ],
   "source": [
    "sentence_a = \"\"\"north korea's rubberstamp parliament has reelected kim jongil as chairman of the national defence commission, the country's most powerful position. mr kim's reelection comes days after a rocket launch that was lauded at home but criticised at the un. analysts say the move will help cement mr kim's position, after rumours he lost some of his grip on power after suffering a stroke in august. observers have expressed concern about the apparent lack of a succession plan.\n",
    "\"\"\"\n",
    "sentence_b = \"kim jongil is the chairman of the un.\"\n",
    "\n",
    "test_sentence_a = \"kim jongil as chairman of the national defence commission\"\n",
    "test_sentence_b = 'kim jongil is the chairman of the un.'\n",
    "\n",
    "model_pretrained.to(device)\n",
    "X_pretrained = explainability_compare(model_pretrained, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_single = explainability_compare(model_single, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_multi = explainability_compare(model_multi, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "\n",
    "print(\"Pretrained: %6f\\nSingleTask: %6f\\nMultiTask: %6f\" % (X_pretrained, X_single, X_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained: 0.015666\n",
      "SingleTask: 0.016491\n",
      "MultiTask: 0.013709\n"
     ]
    }
   ],
   "source": [
    "sentence_a = \"\"\"leftist mauricio funes of el salvador's former marxist rebel fmln party has won the country's presidential election. he defeated his conservative rival, the arena party's rodrigo avila, who has admitted defeat. arena had won every presidential election since the end of el salvador's civil war 18 years ago. addressing jubilant supporters, mr funes said it was the happiest day of his life and the beginning of a new chapter of peace for the country. branded by his opponents as a puppet of venezuala's president hugo chavez, mr funes vowed to respect all salvadorian democratic institutions.\n",
    "\"\"\"\n",
    "sentence_b = \"rodrigo avila has won el salvador's presidential election.\"\n",
    "\n",
    "test_sentence_a = \"\"\"leftist mauricio funes of el salvador's former marxist rebel fmln party has won the country's presidential election\"\"\"\n",
    "test_sentence_b = \"rodrigo avila has won el salvador's presidential election\"\n",
    "\n",
    "model_pretrained.to(device)\n",
    "X_pretrained = explainability_compare(model_pretrained, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_single = explainability_compare(model_single, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_multi = explainability_compare(model_multi, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "\n",
    "print(\"Pretrained: %6f\\nSingleTask: %6f\\nMultiTask: %6f\" % (X_pretrained, X_single, X_multi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained: 0.115151\n",
      "SingleTask: 0.062170\n",
      "MultiTask: 0.113543\n"
     ]
    }
   ],
   "source": [
    "sentence_a = \"\"\"us music producer phil spector has been convicted of murdering actress lana clarkson, after a fivemonth retrial. the 68yearold, famous for the \"wall of sound\" recording technique, faces between 15 years and life in prison. he had pleaded not guilty to the second degree murder of 40yearold ms clarkson, who was shot in the mouth at spector's home in los angeles. spector was remanded in custody until sentencing on 29 may. his lawyer has said he intends to appeal.\n",
    "\"\"\"\n",
    "sentence_b = \"phil spector was a music producer.\"\n",
    "\n",
    "test_sentence_a = \"us music producer phil spector\"\n",
    "test_sentence_b = \"phil spector was a music producer\"\n",
    "\n",
    "model_pretrained.to(device)\n",
    "X_pretrained = explainability_compare(model_pretrained, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_single = explainability_compare(model_single, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_multi = explainability_compare(model_multi, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "\n",
    "print(\"Pretrained: %6f\\nSingleTask: %6f\\nMultiTask: %6f\" % (X_pretrained, X_single, X_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained: 0.015529\n",
      "SingleTask: 0.006583\n",
      "MultiTask: 0.016079\n"
     ]
    }
   ],
   "source": [
    "sentence_a = \"\"\"crippa died a week ago at santa barbara cottage hospital, seven days after he ate a heaping plate of the deadly amanita ocreata mushrooms, said his wife, joan crippa. known as \"death angel\" for its snowwhite appearance, the fungus has deadly toxins that worked their way through crippa's system, sickening him and eventually causing his liver to fail. family members had often warned crippa against indulging in his passion for hunting wild mushrooms, an activity he learned from his italian immigrant parents, his wife said.\n",
    "\"\"\"\n",
    "sentence_b = \"crippa was killed by a wild mushroom.\"\n",
    "\n",
    "test_sentence_a = \"crippa died a week ago at santa barbara cottage hospital, seven days after he ate a heaping plate of the deadly amanita ocreata mushrooms\"\n",
    "test_sentence_b = \"crippa was killed by a wild mushroom\"\n",
    "\n",
    "model_pretrained.to(device)\n",
    "X_pretrained = explainability_compare(model_pretrained, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_single = explainability_compare(model_single, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_multi = explainability_compare(model_multi, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "\n",
    "print(\"Pretrained: %6f\\nSingleTask: %6f\\nMultiTask: %6f\" % (X_pretrained, X_single, X_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained: 0.017591\n",
      "SingleTask: 0.016502\n",
      "MultiTask: 0.023821\n"
     ]
    }
   ],
   "source": [
    "sentence_a = \"\"\"one man, who did not want to be named, said: \"there was a big fight  i heard it was some sort of retaliation after something else that happened earlier on.\" most of larkhall park has been sealed off following the incident. a 100metre stretch of wandsworth road, which runs along the western side of the park, has also been closed to traffic. the teenager's death brings the total number of young people to die in violent circumstances in london this year, to seven.\n",
    "\"\"\"\n",
    "sentence_b = \"seven young people died in violent situations this year in london.\"\n",
    "\n",
    "test_sentence_a = \"the teenager's death brings the total number of young people to die in violent circumstances in london this year, to seven\"\n",
    "test_sentence_b = \"seven young people died in violent situations this year in london\"\n",
    "\n",
    "model_pretrained.to(device)\n",
    "X_pretrained = explainability_compare(model_pretrained, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_single = explainability_compare(model_single, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_multi = explainability_compare(model_multi, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "\n",
    "print(\"Pretrained: %6f\\nSingleTask: %6f\\nMultiTask: %6f\" % (X_pretrained, X_single, X_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained: 0.027014\n",
      "SingleTask: 0.028418\n",
      "MultiTask: 0.029012\n"
     ]
    }
   ],
   "source": [
    "sentence_a = \"\"\"a ugandan spy who set up a bogus charity and embezzled thousands of dollars of funding meant for aids patients has been jailed for 10 years. teddy sseezi cheeye, 51, took $56,000 38,000 from the global fund charity, which aims to prevent hiv, tuberculosis and malaria. he set up an ngo, the uganda centre for accountability, which received cash in 2005 to do hiv/aids community work. but the high court in kampala heard cheeye siphoned off the funds instead.\n",
    "\"\"\"\n",
    "sentence_b = \"teddy sseezi cheeye is an ugandan spy.\"\n",
    "\n",
    "test_sentence_a = \"a ugandan spy who set up a bogus charity and embezzled thousands of dollars of funding meant for aids patients has been jailed for 10 years. teddy sseezi cheeye\"\n",
    "test_sentence_b = \"teddy sseezi cheeye is an ugandan spy\"\n",
    "\n",
    "model_pretrained.to(device)\n",
    "X_pretrained = explainability_compare(model_pretrained, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_single = explainability_compare(model_single, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_multi = explainability_compare(model_multi, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "\n",
    "print(\"Pretrained: %6f\\nSingleTask: %6f\\nMultiTask: %6f\" % (X_pretrained, X_single, X_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained: 0.012499\n",
      "SingleTask: 0.012254\n",
      "MultiTask: 0.022521\n"
     ]
    }
   ],
   "source": [
    "sentence_a = \"\"\"a japanese pop singer who was arrested for public indecency after being found drunk and naked in a tokyo park has apologised for his \"shameful\" conduct. \"i drank a lot and did not know what i was doing,\" tsuyoshi kusanagi said on friday after being released by police. \"i deeply apologise to fans for causing so much trouble and worry,\" he added. kusanagi, 34, shot to fame as a member of 1980s boy band smap. he has not been charged for the incident, which took place in the early hours of thursday.\n",
    "\"\"\"\n",
    "sentence_b = \"tsuyoshi kusanagi is a pop star.\"\n",
    "\n",
    "test_sentence_a = \"\"\"a japanese pop singer who was arrested for public indecency after being found drunk and naked in a tokyo park has apologised for his \"shameful\" conduct. \"i drank a lot and did not know what i was doing,\" tsuyoshi kusanagi said on friday after being released by police\"\"\"\n",
    "test_sentence_b = \"tsuyoshi kusanagi is a pop star\"\n",
    "\n",
    "model_pretrained.to(device)\n",
    "X_pretrained = explainability_compare(model_pretrained, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_single = explainability_compare(model_single, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_multi = explainability_compare(model_multi, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "\n",
    "print(\"Pretrained: %6f\\nSingleTask: %6f\\nMultiTask: %6f\" % (X_pretrained, X_single, X_multi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained: 0.056409\n",
      "SingleTask: 0.057168\n",
      "MultiTask: 0.059053\n"
     ]
    }
   ],
   "source": [
    "sentence_a = \"\"\"a ugandan spy who set up a bogus charity and embezzled thousands of dollars of funding meant for aids patients has been jailed for 10 years. teddy sseezi cheeye, 51, took $56,000 38,000 from the global fund charity, which aims to prevent hiv, tuberculosis and malaria. he set up an ngo, the uganda centre for accountability, which received cash in 2005 to do hiv/aids community work. but the high court in kampala heard cheeye siphoned off the funds instead.\n",
    "\"\"\"\n",
    "sentence_b = \"teddy sseezi cheeye is an ugandan spy.\"\n",
    "\n",
    "test_sentence_a = \"a ugandan spy teddy sseezi cheeye\"\n",
    "test_sentence_b = \"teddy sseezi cheeye is an ugandan spy\"\n",
    "\n",
    "model_pretrained.to(device)\n",
    "X_pretrained = explainability_compare(model_pretrained, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_single = explainability_compare(model_single, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_multi = explainability_compare(model_multi, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "\n",
    "print(\"Pretrained: %6f\\nSingleTask: %6f\\nMultiTask: %6f\" % (X_pretrained, X_single, X_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained: 0.037847\n",
      "SingleTask: 0.036912\n",
      "MultiTask: 0.062019\n"
     ]
    }
   ],
   "source": [
    "sentence_a = \"\"\"a japanese pop singer who was arrested for public indecency after being found drunk and naked in a tokyo park has apologised for his \"shameful\" conduct. \"i drank a lot and did not know what i was doing,\" tsuyoshi kusanagi said on friday after being released by police. \"i deeply apologise to fans for causing so much trouble and worry,\" he added. kusanagi, 34, shot to fame as a member of 1980s boy band smap. he has not been charged for the incident, which took place in the early hours of thursday.\n",
    "\"\"\"\n",
    "sentence_b = \"tsuyoshi kusanagi is a pop star.\"\n",
    "\n",
    "test_sentence_a = \"\"\"a japanese pop singer tsuyoshi kusanagi\"\"\"\n",
    "test_sentence_b = \"tsuyoshi kusanagi is a pop star\"\n",
    "\n",
    "model_pretrained.to(device)\n",
    "X_pretrained = explainability_compare(model_pretrained, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_single = explainability_compare(model_single, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "X_multi = explainability_compare(model_multi, tokenizer, sentence_a, sentence_b, test_sentence_a, test_sentence_b)\n",
    "\n",
    "print(\"Pretrained: %6f\\nSingleTask: %6f\\nMultiTask: %6f\" % (X_pretrained, X_single, X_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
