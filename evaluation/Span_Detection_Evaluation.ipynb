{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir) \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, SequentialSampler, DataLoader\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification, XLNetPreTrainedModel, XLNetModel\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from XLNet import (Dataset_Span_Detection,\n",
    "                   XLNetForMultiSequenceClassification,\n",
    "                   SpanDetectionResult, \n",
    "                   SquadExample,\n",
    "                   SquadFeatures,\n",
    "                   squad_convert_example_to_features)\n",
    "from span_detection_metrics import compute_predictions_log_probs, span_evaluate\n",
    "from utils import *\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.notebook import tqdm, trange\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "dataset = Dataset_Span_Detection(\"RTE5_test_span\", tokenizer=tokenizer)\n",
    "eval_sampler = SequentialSampler(dataset)\n",
    "eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=1)\n",
    "model = torch.load('../3multi_task/multi_0.6, 25, 15.pkl', map_location=torch.device('cpu'))\n",
    "#model = torch.load('../3multi_task/multi_0.6, 25, 15.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c95fcc7b9244e29a2ee2a8d1dd267cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=600, style=ProgressStyle(description_width='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "all_examples = []\n",
    "all_features = []\n",
    "\n",
    "for data in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        task = data[0]\n",
    "        example_index = data[6]\n",
    "        unique_id = data[7]\n",
    "        input_ids, attention_mask, token_type_ids, cls_index, p_mask = [t.squeeze(0).to(device) for t in data[1:6]]\n",
    "        \n",
    "        question_text, context_text, answer_text, start_position_character = [t[0] for t in data[-4:]]\n",
    "\n",
    "        example = SquadExample(\n",
    "            question_text=question_text,\n",
    "            context_text=context_text,\n",
    "            answer_text=answer_text,\n",
    "            start_position_character=start_position_character,\n",
    "            unique_id=unique_id\n",
    "        )\n",
    "\n",
    "        feature = squad_convert_example_to_features(example,\n",
    "                                                    max_seq_length=384,\n",
    "                                                    doc_stride=128,\n",
    "                                                    max_query_length=64,\n",
    "                                                    is_training=False,\n",
    "                                                    example_index=example_index,\n",
    "                                                    unique_id=unique_id,\n",
    "                                                    )\n",
    "        \n",
    "        output = model(input_ids=input_ids, \n",
    "                        token_type_ids=token_type_ids, \n",
    "                        attention_mask=attention_mask, \n",
    "                        cls_index=cls_index,\n",
    "                        p_mask=p_mask,\n",
    "                        task=task)\n",
    "        \n",
    "        #eval_feature = features\n",
    "        \n",
    "#         start_logits = output[0]\n",
    "#         start_top_index = output[1]\n",
    "#         end_logits = output[2]\n",
    "#         end_top_index = output[3]\n",
    "#         cls_logits = output[4]\n",
    "        start_logits, start_top_index, end_logits, end_top_index, cls_logits = attention_weight_span(data, feature, output)\n",
    "        \n",
    "        result = SpanDetectionResult(\n",
    "            unique_id,\n",
    "            start_logits,\n",
    "            end_logits,\n",
    "            start_top_index=start_top_index,\n",
    "            end_top_index=end_top_index,\n",
    "            cls_logits=cls_logits,\n",
    "        )\n",
    "        \n",
    "        all_results.append(result)\n",
    "        all_examples.append(example)\n",
    "        all_features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([0.4354, 0.3759, 0.0914, 0.0453, 0.0204])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0914])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.unsqueeze(1)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4354, 0.3759, 0.0914, 0.0453, 0.0204])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, result in enumerate(all_results):\n",
    "    result.start_logits = result.start_logits.unsqueeze(0)\n",
    "    result.start_top_index = result.start_top_index.unsqueeze(0)\n",
    "    #print(result.end_top_index.size())\n",
    "    #result.end_logits = result.end_logits.unsqueeze(0)\n",
    "    #result.end_top_index = result.end_top_index.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results[0].start_logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_n_top = 5\n",
    "end_n_top = 5\n",
    "n_best_size = 20\n",
    "max_answer_length = 60\n",
    "min_answer_length = 1\n",
    "do_lower_case=False\n",
    "\n",
    "output_dir = \"../evaluation/\"\n",
    "prefix = ''\n",
    "output_prediction_file = os.path.join(output_dir, \"prediction_{}.json\".format(prefix))\n",
    "output_nbest_file = os.path.join(output_dir, \"nbest_predictions_{}.json\".format(prefix))\n",
    "\n",
    "predictions = compute_predictions_log_probs(\n",
    "    all_examples,\n",
    "    all_features,\n",
    "    all_results,\n",
    "    n_best_size,\n",
    "    max_answer_length,\n",
    "    min_answer_length,\n",
    "    output_prediction_file,\n",
    "    output_nbest_file,\n",
    "    start_n_top,\n",
    "    end_n_top,\n",
    "    tokenizer,\n",
    "    verbose_logging=True,\n",
    ")\n",
    "\n",
    "result = span_evaluate(all_examples, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('exact', 0.0),\n",
       "             ('f1', 0.9756985980364193),\n",
       "             ('total', 600),\n",
       "             ('HasAns_exact', 0.0),\n",
       "             ('HasAns_f1', 0.9756985980364193),\n",
       "             ('HasAns_total', 600),\n",
       "             ('best_exact', 0.0),\n",
       "             ('best_exact_thresh', 0.0),\n",
       "             ('best_f1', 0.9756985980364193),\n",
       "             ('best_f1_thresh', 0.0)])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = all_features[0][0].input_ids\n",
    "tokens = all_features[0][0].tokens\n",
    "token_type_ids = all_features[0][0].token_type_ids\n",
    "attention = output[5]\n",
    "attn = format_attention(attention, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_b_start = token_type_ids.index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_a = slice(0, sentence_b_start)\n",
    "slice_b = slice(sentence_b_start, len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attn_data = attn[:, :, slice_a, slice_b]\n",
    "sentence_a_tokens = tokens[slice_a]\n",
    "sentence_b_tokens = tokens[slice_b]\n",
    "pair = pair_match_accumulation(sentence_a_tokens, sentence_b_tokens, attn_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_weight_span(data, feature, output):\n",
    "    \n",
    "    sentence_a = data[9]\n",
    "    sentence_a = data[8]\n",
    "    \n",
    "    input_ids = feature[0].input_ids\n",
    "    tokens = feature[0].tokens\n",
    "    token_type_ids = feature[0].token_type_ids\n",
    "    \n",
    "    attention = output[5]\n",
    "    attn = format_attention(attention, tokens)\n",
    "    \n",
    "    sentence_b_start = token_type_ids.index(1)\n",
    "    slice_a = slice(0, sentence_b_start)\n",
    "    slice_b = slice(sentence_b_start, len(tokens))\n",
    "    \n",
    "    attn_data = attn[:, :, slice_a, slice_b]\n",
    "    sentence_a_tokens = tokens[slice_a]\n",
    "    sentence_b_tokens = tokens[slice_b]\n",
    "    attn_score = pair_match_accumulation(sentence_a_tokens, sentence_b_tokens, attn_data)\n",
    "    \n",
    "    attn_score = torch.tensor(attn_score)\n",
    "    start_log_probs = F.softmax(attn_score, dim=-1)\n",
    "    start_top_log_probs, start_top_index = torch.topk(start_log_probs, 5, dim=-1)\n",
    "    end_top_log_probs, end_top_index = torch.topk(start_log_probs, 25, dim=-1)\n",
    "    cls_logits = 0\n",
    "    \n",
    "    return (start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = torch.tensor(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_log_probs = F.softmax(pair, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7767e-10, 1.0510e-10, 3.7801e-10, 6.9056e-11, 5.4780e-10])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_log_probs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_log_probs[start_top_index] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.2611e-11,\n",
       "        5.4890e-10, 1.2520e-10, 5.1879e-11, 2.0303e-09, 4.1818e-10, 7.0310e-11,\n",
       "        1.2945e-09, 1.0122e-09, 4.8054e-10, 7.8518e-10, 1.7207e-10, 9.3816e-10,\n",
       "        2.0141e-09, 1.5369e-10, 6.0906e-10, 1.1986e-09, 1.6682e-10, 2.9706e-10,\n",
       "        4.2920e-10, 5.1539e-10, 3.1197e-10, 1.5980e-10, 6.3291e-09, 4.0786e-10,\n",
       "        2.9646e-10, 5.1870e-09, 1.6672e-09, 1.3115e-09, 2.7920e-10, 3.9995e-09,\n",
       "        1.5514e-09, 1.0042e-09, 8.7124e-10, 1.2626e-09, 4.4290e-09, 9.6287e-10,\n",
       "        5.0086e-09, 2.2250e-10, 6.0759e-11, 1.5261e-10, 4.7576e-10, 1.2646e-10,\n",
       "        4.5102e-11, 1.6375e-09, 8.6776e-10, 1.8611e-09, 4.7815e-10, 1.2971e-09,\n",
       "        8.8706e-10, 7.2120e-10, 3.9344e-10, 3.8664e-11, 2.9351e-10, 1.3940e-09,\n",
       "        1.6434e-10, 5.4671e-10, 4.6032e-10, 9.2937e-11, 1.1903e-09, 3.2373e-10,\n",
       "        1.1408e-10, 8.5143e-10, 7.5289e-10, 1.3590e-10, 7.3915e-11, 1.6900e-10,\n",
       "        2.9765e-10, 8.9598e-10, 1.9479e-10, 8.8264e-10, 4.6448e-10, 7.4094e-10,\n",
       "        4.1485e-10, 7.7938e-11, 1.3746e-09, 5.3161e-10, 4.9716e-10, 1.5774e-10,\n",
       "        1.2966e-10, 6.0421e-10, 6.7216e-11, 3.6963e-11, 4.3006e-10, 1.3851e-10,\n",
       "        1.5338e-10, 3.9882e-11, 2.6906e-10, 9.1188e-11, 3.4238e-10, 5.7245e-10,\n",
       "        3.8565e-10, 1.3644e-10, 1.7892e-10, 3.1210e-09, 4.8781e-10, 1.2058e-09,\n",
       "        1.5201e-10, 7.9275e-11, 4.9567e-10, 2.3135e-10, 5.6675e-10, 4.3699e-10,\n",
       "        1.6115e-09, 2.2836e-10, 2.4961e-10, 4.7576e-10, 1.3906e-10, 2.6955e-08,\n",
       "        3.5629e-08, 3.0974e-08, 5.3419e-08, 8.9047e-08, 4.6201e-06, 4.3059e-07,\n",
       "        1.8675e-08, 1.1169e-08, 1.0861e-08, 1.5346e-04, 7.3362e-05, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 4.4693e-04, 1.5813e-04, 6.7953e-04,\n",
       "        4.1654e-05, 3.2104e-06, 1.2642e-06, 1.2858e-06, 0.0000e+00, 3.0787e-13])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_top_log_probs, start_top_index = torch.topk(pair_log_probs, 5, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_top_log_probs, end_top_index = torch.topk(pair_log_probs, 25, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([131, 129, 130, 123, 124, 132, 118, 133, 135, 134, 119, 117, 116, 114,\n",
       "        115, 113, 120, 121, 122,  28,  31,  42,  40,  35,  99])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_top_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([136, 127, 128, 125, 126])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_top_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cls_logits': 0,\n",
       " 'end_logits': tensor([3.2124e-01, 2.7649e-01, 1.2141e-01, 8.4030e-02, 5.3260e-02, 4.0092e-02,\n",
       "         3.2989e-02, 2.4463e-02, 2.2336e-02, 1.3766e-02, 3.0440e-03, 1.8555e-03,\n",
       "         1.7060e-03, 1.3181e-03, 6.5651e-04, 6.4933e-04, 2.0954e-04, 1.6319e-04,\n",
       "         5.6483e-05, 5.0802e-05, 2.9516e-05, 2.2918e-05, 2.1284e-05, 1.0665e-05,\n",
       "         9.6691e-06]),\n",
       " 'end_top_index': tensor([ 16,  21,  18,  22,  23,  20,  19,  85,  14,  17,  87,  88, 136,  84,\n",
       "          83,  86,  15,  80,   5,   6,  82,  13,  81,   4, 134]),\n",
       " 'start_logits': tensor([0.3212, 0.2765, 0.1214, 0.0840, 0.0533]),\n",
       " 'start_top_index': tensor([16, 21, 18, 22, 23]),\n",
       " 'unique_id': tensor([1000001])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = SpanDetectionResult(\n",
    "            100001,\n",
    "            start_top_log_probs,\n",
    "            end_top_log_probs,\n",
    "            start_top_index=start_top_index,\n",
    "            end_top_index=end_top_index,\n",
    "            cls_logits=0,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
