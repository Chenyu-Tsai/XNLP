{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir) \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, SequentialSampler, DataLoader\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification, XLNetPreTrainedModel, XLNetModel\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from XLNet import (Dataset_Span_Detection,\n",
    "                   XLNetForMultiSequenceClassification,\n",
    "                   SpanDetectionResult, \n",
    "                   SquadExample,\n",
    "                   SquadFeatures,\n",
    "                   squad_convert_example_to_features)\n",
    "# from span_detection_metrics import compute_predictions_log_probs, span_evaluate\n",
    "from span_detection_metrics import *\n",
    "\n",
    "from utils import *\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.notebook import tqdm, trange\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "dataset = Dataset_Span_Detection(\"RTE5_test_span\", tokenizer=tokenizer)\n",
    "eval_sampler = SequentialSampler(dataset)\n",
    "eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=1)\n",
    "#model = XLNetForMultiSequenceClassification.from_pretrained(\"xlnet-base-cased\", output_attentions=True,)\n",
    "model = torch.load('../3multi_task/0513-3multi_0.61, 38.9, 7.pkl', map_location=torch.device('cpu'))\n",
    "#model = torch.load('../2multi_task/2multi_0.61, 22.2.pkl', map_location=torch.device('cpu'))\n",
    "#model = torch.load('../single_task/0512_single_task_0.61, 17.8.pkl', map_location=torch.device('cpu'))\n",
    "#model = torch.load('../3multi_task/multi_0.6, 25, 15.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ad778a5a284f959605cfe0f19825ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=600, style=ProgressStyle(description_width='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenyutsai/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "all_examples = []\n",
    "all_features = []\n",
    "\n",
    "for data in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        task = data[0]\n",
    "        example_index = data[6]\n",
    "        unique_id = data[7]\n",
    "        input_ids, attention_mask, token_type_ids, cls_index, p_mask = [t.squeeze(0).to(device) for t in data[1:6]]\n",
    "        \n",
    "        question_text, context_text, answer_text, start_position_character, label = [t[0] for t in data[-5:]]\n",
    "        \n",
    "        \n",
    "        \n",
    "        output = model(input_ids=input_ids, \n",
    "                       token_type_ids=token_type_ids, \n",
    "                       attention_mask=attention_mask, \n",
    "                       cls_index=cls_index,\n",
    "                       p_mask=p_mask,\n",
    "                       task=task)\n",
    "        eval_task = 0\n",
    "        outputs_3way = model(input_ids=input_ids,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            task=eval_task,\n",
    "                           )\n",
    "        logits = outputs_3way[0]\n",
    "        _, pred = torch.max(logits.data, 1)\n",
    "        pred = pred if pred == label else None\n",
    "        \n",
    "        example = SquadExample(\n",
    "            question_text=question_text,\n",
    "            context_text=context_text,\n",
    "            answer_text=answer_text,\n",
    "            start_position_character=start_position_character,\n",
    "            unique_id=unique_id,\n",
    "            pred=pred,\n",
    "        )\n",
    "        \n",
    "        feature = squad_convert_example_to_features(example,\n",
    "                                                    max_seq_length=384,\n",
    "                                                    doc_stride=128,\n",
    "                                                    max_query_length=64,\n",
    "                                                    is_training=False,\n",
    "                                                    example_index=example_index,\n",
    "                                                    unique_id=unique_id,\n",
    "                                                    )\n",
    "        \n",
    "        #eval_feature = features\n",
    "        \n",
    "#         start_logits = output[0]\n",
    "#         start_top_index = output[1]\n",
    "#         end_logits = output[2]\n",
    "#         end_top_index = output[3]\n",
    "#         cls_logits = output[4]\n",
    "        #top_n = top\n",
    "        start_logits, start_top_index, end_logits, end_top_index, cls_logits, top_n = attention_weight_span(data, feature, output)\n",
    "        \n",
    "        result = SpanDetectionResult(\n",
    "            unique_id,\n",
    "            start_logits.unsqueeze(0),\n",
    "            end_logits,\n",
    "            start_top_index=start_top_index.unsqueeze(0),\n",
    "            end_top_index=end_top_index,\n",
    "            cls_logits=cls_logits,\n",
    "            top_n=top_n\n",
    "        )\n",
    "        \n",
    "        all_results.append(result)\n",
    "        all_examples.append(example)\n",
    "        all_features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('exact', 16.0),\n",
       "             ('f1', 65.19),\n",
       "             ('total', 600),\n",
       "             ('Entailment_exact', 12.82),\n",
       "             ('Entailment_f1', 70.44),\n",
       "             ('Entailment_accuracy', 26.0),\n",
       "             ('Neutral_exact', 6.17),\n",
       "             ('Neutral_f1', 50.34),\n",
       "             ('Neutral_accuracy', 38.57),\n",
       "             ('Contradiction_exact', 21.95),\n",
       "             ('Contradiction_f1', 58.45),\n",
       "             ('Contradiction_accuracy', 45.56)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_n_top = 5\n",
    "end_n_top = 10\n",
    "n_best_size = 5\n",
    "max_answer_length = 50\n",
    "min_answer_length = 5\n",
    "do_lower_case=False\n",
    "\n",
    "output_dir = \"../evaluation/\"\n",
    "#prefix = '50-5, 3Tasks_3-46'\n",
    "#prefix = '3tasks_1%'\n",
    "prefix = 'pretrained'\n",
    "\n",
    "output_prediction_file = os.path.join(output_dir, \"prediction_{}.json\".format(prefix))\n",
    "output_nbest_file = os.path.join(output_dir, \"nbest_predictions_{}.json\".format(prefix))\n",
    "output_best_answers_file = os.path.join(output_dir, \"best_answers_{}.json\".format(prefix))\n",
    "\n",
    "predictions = compute_predictions_log_probs(\n",
    "    all_examples,\n",
    "    all_features,\n",
    "    all_results,\n",
    "    n_best_size,\n",
    "    max_answer_length,\n",
    "    min_answer_length,\n",
    "    output_prediction_file,\n",
    "    output_nbest_file,\n",
    "    start_n_top,\n",
    "    end_n_top,\n",
    "    tokenizer,\n",
    "    verbose_logging=True,\n",
    ")\n",
    "\n",
    "result = span_evaluate(all_examples, predictions, output_best_answers_file)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../2multi_task/2multi_0.61, 22.2.pkl', map_location=torch.device('cpu'))\n",
    "#model = torch.load('../single_task/0512_single_task_0.61, 17.8.pkl', map_location=torch.device('cpu'))\n",
    "#model = torch.load('../3multi_task/multi_0.6, 25, 15.pkl')\n",
    "\n",
    "all_results = []\n",
    "all_examples = []\n",
    "all_features = []\n",
    "\n",
    "for data in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        task = data[0]\n",
    "        example_index = data[6]\n",
    "        unique_id = data[7]\n",
    "        input_ids, attention_mask, token_type_ids, cls_index, p_mask = [t.squeeze(0).to(device) for t in data[1:6]]\n",
    "        \n",
    "        question_text, context_text, answer_text, start_position_character, label = [t[0] for t in data[-5:]]\n",
    "        \n",
    "        \n",
    "        \n",
    "        output = model(input_ids=input_ids, \n",
    "                       token_type_ids=token_type_ids, \n",
    "                       attention_mask=attention_mask, \n",
    "                       cls_index=cls_index,\n",
    "                       p_mask=p_mask,\n",
    "                       task=task)\n",
    "        eval_task = 0\n",
    "        outputs_3way = model(input_ids=input_ids,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            task=eval_task,\n",
    "                           )\n",
    "        logits = outputs_3way[0]\n",
    "        _, pred = torch.max(logits.data, 1)\n",
    "        pred = pred if pred == label else None\n",
    "        \n",
    "        example = SquadExample(\n",
    "            question_text=question_text,\n",
    "            context_text=context_text,\n",
    "            answer_text=answer_text,\n",
    "            start_position_character=start_position_character,\n",
    "            unique_id=unique_id,\n",
    "            pred=pred,\n",
    "        )\n",
    "        \n",
    "        feature = squad_convert_example_to_features(example,\n",
    "                                                    max_seq_length=384,\n",
    "                                                    doc_stride=128,\n",
    "                                                    max_query_length=64,\n",
    "                                                    is_training=False,\n",
    "                                                    example_index=example_index,\n",
    "                                                    unique_id=unique_id,\n",
    "                                                    )\n",
    "        \n",
    "        #eval_feature = features\n",
    "        \n",
    "#         start_logits = output[0]\n",
    "#         start_top_index = output[1]\n",
    "#         end_logits = output[2]\n",
    "#         end_top_index = output[3]\n",
    "#         cls_logits = output[4]\n",
    "        start_logits, start_top_index, end_logits, end_top_index, cls_logits, top_n = attention_weight_span(data, feature, output)\n",
    "        \n",
    "        result = SpanDetectionResult(\n",
    "            unique_id,\n",
    "            start_logits.unsqueeze(0),\n",
    "            end_logits,\n",
    "            start_top_index=start_top_index.unsqueeze(0),\n",
    "            end_top_index=end_top_index,\n",
    "            cls_logits=cls_logits,\n",
    "            top_n=top_n\n",
    "        )\n",
    "        \n",
    "        all_results.append(result)\n",
    "        all_examples.append(example)\n",
    "        all_features.append(feature)\n",
    "        \n",
    "start_n_top = 5\n",
    "end_n_top = 10\n",
    "n_best_size = 5\n",
    "max_answer_length = 50\n",
    "min_answer_length = 5\n",
    "do_lower_case=False\n",
    "\n",
    "output_dir = \"../evaluation/\"\n",
    "#prefix = '50-5, 3Tasks_3-46'\n",
    "#prefix = '3tasks_1%'\n",
    "prefix = '2tasks'\n",
    "\n",
    "output_prediction_file = os.path.join(output_dir, \"prediction_{}.json\".format(prefix))\n",
    "output_nbest_file = os.path.join(output_dir, \"nbest_predictions_{}.json\".format(prefix))\n",
    "output_best_answers_file = os.path.join(output_dir, \"best_answers_{}.json\".format(prefix))\n",
    "\n",
    "predictions = compute_predictions_log_probs(\n",
    "    all_examples,\n",
    "    all_features,\n",
    "    all_results,\n",
    "    n_best_size,\n",
    "    max_answer_length,\n",
    "    min_answer_length,\n",
    "    output_prediction_file,\n",
    "    output_nbest_file,\n",
    "    start_n_top,\n",
    "    end_n_top,\n",
    "    tokenizer,\n",
    "    verbose_logging=True,\n",
    ")\n",
    "\n",
    "result = span_evaluate(all_examples, predictions, output_best_answers_file)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../single_task/0512_single_task_0.61, 17.8.pkl', map_location=torch.device('cpu'))\n",
    "#model = torch.load('../3multi_task/multi_0.6, 25, 15.pkl')\n",
    "\n",
    "all_results = []\n",
    "all_examples = []\n",
    "all_features = []\n",
    "\n",
    "for data in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        task = data[0]\n",
    "        example_index = data[6]\n",
    "        unique_id = data[7]\n",
    "        input_ids, attention_mask, token_type_ids, cls_index, p_mask = [t.squeeze(0).to(device) for t in data[1:6]]\n",
    "        \n",
    "        question_text, context_text, answer_text, start_position_character, label = [t[0] for t in data[-5:]]\n",
    "        \n",
    "        \n",
    "        \n",
    "        output = model(input_ids=input_ids, \n",
    "                       token_type_ids=token_type_ids, \n",
    "                       attention_mask=attention_mask, \n",
    "                       cls_index=cls_index,\n",
    "                       p_mask=p_mask,\n",
    "                       task=task)\n",
    "        eval_task = 0\n",
    "        outputs_3way = model(input_ids=input_ids,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            task=eval_task,\n",
    "                           )\n",
    "        logits = outputs_3way[0]\n",
    "        _, pred = torch.max(logits.data, 1)\n",
    "        pred = pred if pred == label else None\n",
    "        \n",
    "        example = SquadExample(\n",
    "            question_text=question_text,\n",
    "            context_text=context_text,\n",
    "            answer_text=answer_text,\n",
    "            start_position_character=start_position_character,\n",
    "            unique_id=unique_id,\n",
    "            pred=pred,\n",
    "        )\n",
    "        \n",
    "        feature = squad_convert_example_to_features(example,\n",
    "                                                    max_seq_length=384,\n",
    "                                                    doc_stride=128,\n",
    "                                                    max_query_length=64,\n",
    "                                                    is_training=False,\n",
    "                                                    example_index=example_index,\n",
    "                                                    unique_id=unique_id,\n",
    "                                                    )\n",
    "        \n",
    "        #eval_feature = features\n",
    "        \n",
    "#         start_logits = output[0]\n",
    "#         start_top_index = output[1]\n",
    "#         end_logits = output[2]\n",
    "#         end_top_index = output[3]\n",
    "#         cls_logits = output[4]\n",
    "        start_logits, start_top_index, end_logits, end_top_index, cls_logits, top_n = attention_weight_span(data, feature, output)\n",
    "        \n",
    "        result = SpanDetectionResult(\n",
    "            unique_id,\n",
    "            start_logits.unsqueeze(0),\n",
    "            end_logits,\n",
    "            start_top_index=start_top_index.unsqueeze(0),\n",
    "            end_top_index=end_top_index,\n",
    "            cls_logits=cls_logits,\n",
    "            top_n=top_n\n",
    "        )\n",
    "        \n",
    "        all_results.append(result)\n",
    "        all_examples.append(example)\n",
    "        all_features.append(feature)\n",
    "        \n",
    "start_n_top = 5\n",
    "end_n_top = 10\n",
    "n_best_size = 5\n",
    "max_answer_length = 50\n",
    "min_answer_length = 5\n",
    "do_lower_case=False\n",
    "\n",
    "output_dir = \"../evaluation/\"\n",
    "#prefix = '50-5, 3Tasks_3-46'\n",
    "#prefix = '3tasks_1%'\n",
    "prefix = '1task'\n",
    "\n",
    "output_prediction_file = os.path.join(output_dir, \"prediction_{}.json\".format(prefix))\n",
    "output_nbest_file = os.path.join(output_dir, \"nbest_predictions_{}.json\".format(prefix))\n",
    "output_best_answers_file = os.path.join(output_dir, \"best_answers_{}.json\".format(prefix))\n",
    "\n",
    "predictions = compute_predictions_log_probs(\n",
    "    all_examples,\n",
    "    all_features,\n",
    "    all_results,\n",
    "    n_best_size,\n",
    "    max_answer_length,\n",
    "    min_answer_length,\n",
    "    output_prediction_file,\n",
    "    output_nbest_file,\n",
    "    start_n_top,\n",
    "    end_n_top,\n",
    "    tokenizer,\n",
    "    verbose_logging=True,\n",
    ")\n",
    "\n",
    "result = span_evaluate(all_examples, predictions, output_best_answers_file)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
